{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/the-art-of-tokenization?scriptVersionId=270740837\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2e44b6c4","metadata":{"papermill":{"duration":0.004912,"end_time":"2025-10-25T10:14:19.898905","exception":false,"start_time":"2025-10-25T10:14:19.893993","status":"completed"},"tags":[]},"source":["# The Art of Tokenization, Part 1: Byte Pair Encoding"]},{"cell_type":"markdown","id":"10867a3a","metadata":{"papermill":{"duration":0.003771,"end_time":"2025-10-25T10:14:19.90658","exception":false,"start_time":"2025-10-25T10:14:19.902809","status":"completed"},"tags":[]},"source":["Welcome to this series on the art and science of tokenization! In Natural Language Processing (NLP), before a powerful model like GPT or BERT can understand human language, we must first break that text down into pieces it can recognize. These pieces are called tokens. The process of creating them is tokenization, and it’s one of the most fundamental steps in any NLP pipeline.\n","\n","This series will explore the different strategies for tokenization. We’ll start with one of the most influential methods in the modern NLP landscape: Byte Pair Encoding (BPE)."]},{"cell_type":"markdown","id":"1f962c43","metadata":{"papermill":{"duration":0.003905,"end_time":"2025-10-25T10:14:19.914845","exception":false,"start_time":"2025-10-25T10:14:19.91094","status":"completed"},"tags":[]},"source":["# What is BPE? The “Happy Medium” of Tokenization\n","At its core, Byte Pair Encoding (BPE) is a subword tokenization algorithm. Instead of forcing us to choose between whole words or individual characters, it finds a “happy medium.”\n"]},{"cell_type":"markdown","id":"dca88fa7","metadata":{"papermill":{"duration":0.003394,"end_time":"2025-10-25T10:14:19.922065","exception":false,"start_time":"2025-10-25T10:14:19.918671","status":"completed"},"tags":[]},"source":["Imagine trying to create a dictionary for a language model:\n","* Option A: Word Dictionary. You include every single word (\"cat\", \"run\", \"photosynthesis\", \"antidisestablishmentarianism\").\n","\n","* Problem: The dictionary becomes enormous. What about new words (“de-platforming”), slang (“yeet”), or simple typos (“helllo”)? They are all “Out-of-Vocabulary” (OOV) and become a meaningless <UNK> (unknown) token.\n","\n","* Option B: Character Dictionary. You only include characters ('a', 'b', 'c', '!').\n","\n","* Problem: No more OOV issues, but the text sequences become incredibly long. The sentence “Hello world” is now 11 tokens ('H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'). This makes it computationally expensive and harder for the model to find meaning."]},{"cell_type":"markdown","id":"2ab792bf","metadata":{"papermill":{"duration":0.003389,"end_time":"2025-10-25T10:14:19.929091","exception":false,"start_time":"2025-10-25T10:14:19.925702","status":"completed"},"tags":[]},"source":["This way, a common word like \"where\" can be a single token, while a rare, complex word like \"retrofitting\" can be broken down into meaningful pieces like [\"retro\", \"fit\", \"ting\"]. Crucially, no word is ever “unknown.” Any new word can be built from these subword pieces."]},{"cell_type":"markdown","id":"3148f0ad","metadata":{"papermill":{"duration":0.003978,"end_time":"2025-10-25T10:14:19.936796","exception":false,"start_time":"2025-10-25T10:14:19.932818","status":"completed"},"tags":[]},"source":["# How the BPE Algorithm Works (Conceptually)\n","BPE was originally a data compression algorithm. Its adaptation for NLP is based on a simple, greedy, and iterative idea:\n","> Core Logic: Continuously find the most common pair of adjacent tokens in your text and merge them into a single, new token.\n"]},{"cell_type":"markdown","id":"c4f98197","metadata":{"papermill":{"duration":0.00332,"end_time":"2025-10-25T10:14:19.943899","exception":false,"start_time":"2025-10-25T10:14:19.940579","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">How the BPE Algorithm Works (Conceptually)</h3>\n","    <p style=\"color: #a0aec0;\">BPE is based on a simple, greedy, and iterative idea: continuously find the most common pair of adjacent tokens and merge them. Let's trace the process.</p>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n","<div style=\"margin-top: 25px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 0: Initialize</h4>\n","        <p style=\"margin: 0;\">\n","            First, we break every word down into its basic characters. We also add a special symbol, like <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>, to mark the end of a word. This is important to distinguish between <span class=\"bpe-kbd-dark\">er</span> inside a word (like in “newest”) and <span class=\"bpe-kbd-dark\">er</span> at the end of a word (like in “lower”).\n","        </p>\n","        <p style=\"margin-top: 10px;\">\n","            Our initial “tokens” are just characters: <span class=\"bpe-kbd-dark\">l</span> <span class=\"bpe-kbd-dark\">o</span> <span class=\"bpe-kbd-dark\">w</span> <span class=\"bpe-kbd-dark\">e</span> <span class=\"bpe-kbd-dark\">r</span> <span class=\"bpe-kbd-dark\">n</span> <span class=\"bpe-kbd-dark\">s</span> <span class=\"bpe-kbd-dark\">t</span> <span class=\"bpe-kbd-dark\">i</span> <span class=\"bpe-kbd-dark\">d</span> <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>.\n","        </p>\n","    </div>\n","\n","<div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 1: First Merge</h4>\n","        <p>The algorithm scans the entire corpus and counts the frequency of every adjacent pair of tokens.</p>\n","        <p>Let’s say it finds that the pair <span class=\"bpe-kbd-dark\">e</span> followed by <span class=\"bpe-kbd-dark\">r</span> (i.e., <span class=\"bpe-kbd-dark\">e r</span>) is the most common combination (appearing in <em>lower</em> and <em>wider</em>).</p>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Action:</strong> It merges <span class=\"bpe-kbd-dark\">e</span> and <span class=\"bpe-kbd-dark\">r</span> to create a new token, <span class=\"bpe-kbd-dark\">er</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Vocabulary:</strong> Our vocabulary now includes <span class=\"bpe-kbd-dark\">er</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Corpus Update:</strong> All instances of <span class=\"bpe-kbd-dark\">e r</span> are replaced. So, <span class=\"bpe-kbd-dark\">l o w e r &lt;/w&gt;</span> becomes <span class=\"bpe-kbd-dark\">l o w er &lt;/w&gt;</span>.</li>\n","        </ul>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 2: Second Merge</h4>\n","        <p>The process repeats. The algorithm scans the <em>updated</em> corpus. Perhaps it now finds that <span class=\"bpe-kbd-dark\">er</span> followed by <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span> (i.e., <span class=\"bpe-kbd-dark\">er &lt;/w&gt;</span>) is the most frequent pair.</p>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Action:</strong> It merges <span class=\"bpe-kbd-dark\">er</span> and <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span> to create a new token, <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Vocabulary:</strong> Our vocabulary now includes <span class=\"bpe-kbd-dark\">er</span> and <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Corpus Update:</strong> <span class=\"bpe-kbd-dark\">l o w er &lt;/w&gt;</span> becomes <span class=\"bpe-kbd-dark\">l o w er&lt;/w&gt;</span>.</li>\n","        </ul>\n","    </div>\n","    \n"," <div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 3: Keep Going…</h4>\n","        <p>Next, maybe <span class=\"bpe-kbd-dark\">l</span> followed by <span class=\"bpe-kbd-dark\">o</span> (<span class=\"bpe-kbd-dark\">l o</span>) is the most common. They get merged into <span class=\"bpe-kbd-dark\">lo</span>. Then <span class=\"bpe-kbd-dark\">lo</span> and <span class=\"bpe-kbd-dark\">w</span> get merged into <span class=\"bpe-kbd-dark\">low</span>.</p>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">The End Result</h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This process is repeated for a predetermined number of merges (e.g., <strong>30,000 times</strong>). The final vocabulary consists of the initial characters plus all the new tokens created during the merges. This gives us a ranked list of merge rules that we can use to tokenize any new text.</p>\n","    </div>\n","</div>\n"]},{"cell_type":"markdown","id":"f5e8605f","metadata":{"papermill":{"duration":0.003372,"end_time":"2025-10-25T10:14:19.950898","exception":false,"start_time":"2025-10-25T10:14:19.947526","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","  <h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">\n","    Pros and Cons of BPE\n","  </h3>\n","  <p style=\"color: #a0aec0;\">\n","    BPE is widely used for a reason, but it’s not without its trade‑offs.\n","  </p>\n","\n","  <style>\n","    .bpe-kbd-dark {\n","      background-color: #1a202c;\n","      border: 1px solid #4a5568;\n","      border-bottom: 2px solid #718096;\n","      border-radius: 4px;\n","      padding: 3px 6px;\n","      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","      font-size: 0.9em;\n","      color: #cbd5e1;\n","      white-space: nowrap;\n","    }\n","  </style>\n","\n","  <!-- ✅ PROS SECTION -->\n","  <div style=\"margin-top: 25px;\">\n","    <h4 style=\"color: #68d391; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      Advantages (Pros) 👍\n","    </h4>\n","    <ul style=\"list-style-type: '✔️'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Eliminates Out‑of‑Vocabulary (OOV) Words:</strong> Any new or rare word can be decomposed into a sequence of known subword tokens. The model never has to deal with a completely \n","        <span class=\"bpe-kbd-dark\">&lt;UNK&gt;</span> token.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Controllable Vocabulary Size:</strong> The final vocabulary size is a hyperparameter — the number of merges. You can balance model size and expressiveness.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Efficiency:</strong> Common words stay as single tokens, making sequences shorter and inference faster. Rare words are decomposed smartly, making better use of vocab space.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Captures Morphology:</strong> Learns meaningful parts like prefixes (<span class=\"bpe-kbd-dark\">un-</span>), suffixes (<span class=\"bpe-kbd-dark\">-ing</span>), and stems, helping generalization. If it learns \n","        <span class=\"bpe-kbd-dark\">un</span>, it can recognize \n","        <em>unhappy</em>, <em>unclear</em>, <em>unbelievable</em>.\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- ❌ CONS SECTION -->\n","  <div style=\"margin-top: 35px; border-top: 1px dashed #4a5568; padding-top: 25px;\">\n","    <h4 style=\"color: #f56565; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      Disadvantages (Cons) 👎\n","    </h4>\n","    <ul style=\"list-style-type: '⚠️'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Greedy Approach:</strong> BPE makes local (greedy) merges — not necessarily globally optimal. A different merge order might yield a better vocabulary.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Sensitive to Training Data:</strong> Merge rules depend entirely on the training corpus. A BPE trained on Wikipedia would tokenize Twitter slang or biomedical text poorly.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>The “Byte” Misnomer:</strong> Most modern BPE operates on characters, not raw bytes. True byte‑level BPE (used in GPT‑2) works directly on UTF‑8 bytes and is more robust — handling any text, emoji, or noise.\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- 🌍 CONTEXT SECTION -->\n","  <div style=\"margin-top: 35px; border-top: 1px dashed #4a5568; padding-top: 25px;\">\n","    <h4 style=\"color: #b794f4; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      BPE’s Place in the Tokenization World\n","    </h4>\n","    <p style=\"color: #a0aec0;\">\n","      BPE is a cornerstone method for subword tokenization, but it’s not alone. Its relatives include:\n","    </p>\n","    <ul style=\"list-style-type: '🔹'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>WordPiece</strong> (<span class=\"bpe-kbd-dark\">BERT</span>, <span class=\"bpe-kbd-dark\">DistilBERT</span>): Similar to BPE but merges based on the merge that <em>maximizes the likelihood</em> of the data instead of raw frequency.\n","      </li>\n","      <li>\n","        <strong>Unigram Language Model</strong> (<span class=\"bpe-kbd-dark\">T5</span>, <span class=\"bpe-kbd-dark\">ALBERT</span>, <span class=\"bpe-kbd-dark\">XLNet</span>): A probabilistic approach that prunes subwords contributing least to total corpus probability — can produce multiple valid tokenizations per word (useful as regularization).\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- ⚡ FINAL NOTE -->\n","  <div style=\"margin-top: 30px; background-color: #1a202c; border-left: 5px solid #3182ce; padding: 15px; border-radius: 6px;\">\n","    <p style=\"margin: 0; color: #a0aec0;\">\n","      Now that we understand the theory, let’s roll up our sleeves and build a \n","      <strong style=\"color:#63b3ed;\">BPE tokenizer</strong> from scratch 🔧\n","    </p>\n","  </div>\n","</div>\n"]},{"cell_type":"markdown","id":"dc5f116f","metadata":{"papermill":{"duration":0.003695,"end_time":"2025-10-25T10:14:19.958282","exception":false,"start_time":"2025-10-25T10:14:19.954587","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 1: Preparing the Training Corpus</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        Before we can start merging pairs, we need a corpus of text in the right format. The BPE algorithm operates on a vocabulary built from words and their frequencies. So, our first step is to:\n","    </p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Take a raw text corpus.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Split it into individual words and count the frequency of each unique word.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Represent each word as a sequence of characters, separated by spaces, and add a special end-of-word marker, like <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>.</li>\n","    </ol>\n","     <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This marker is crucial as it allows the algorithm to distinguish between a subword found inside a larger word (like <span class=\"bpe-kbd-dark\">er</span> in \"newer\") and a subword that forms the end of a word (like <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span> in \"lower\"). Our goal is to transform a block of text into a dictionary where keys are the character-split words and values are their frequencies.\n","    </p>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">Example</h4>\n","         <p style=\"margin: 0; color: #a0aec0;\">\n","            <strong>Input Corpus:</strong> <code>\"low lower newest wider low\"</code><br>\n","            <strong>Becomes:</strong> <code>{'l o w </w>': 2, 'l o w e r </w>': 1, ...}</code>\n","        </p>\n","    </div>\n","</div>\n"]},{"cell_type":"code","execution_count":1,"id":"92c4dfe7","metadata":{"execution":{"iopub.execute_input":"2025-10-25T10:14:19.967627Z","iopub.status.busy":"2025-10-25T10:14:19.967137Z","iopub.status.idle":"2025-10-25T10:14:19.979719Z","shell.execute_reply":"2025-10-25T10:14:19.978104Z"},"papermill":{"duration":0.020022,"end_time":"2025-10-25T10:14:19.9821","exception":false,"start_time":"2025-10-25T10:14:19.962078","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Initial Vocabulary ---\n","'l o w </w>': 3\n","'l o w e r </w>': 1\n","'n e w e s t </w>': 1\n","'w i d e r </w>': 1\n"]}],"source":["import collections\n","\n","def get_vocab(corpus: str) -> dict:\n","    \"\"\"\n","    Takes a raw text corpus and creates an initial vocabulary.\n","    \n","    The vocabulary maps each word (split into characters and with an end-of-word marker)\n","    to its frequency in the corpus.\n","\n","    Args:\n","        corpus (str): A string containing the training text.\n","\n","    Returns:\n","        dict: A dictionary where keys are space-separated characters of words\n","              (e.g., 'l o w </w>') and values are their counts.\n","    \"\"\"\n","    # Split the corpus into words and count their frequencies\n","    words = corpus.strip().split()\n","    word_counts = collections.Counter(words)\n","    \n","    # Initialize the vocabulary dictionary\n","    vocab = {}\n","    \n","    # For each word and its count, format it for BPE\n","    for word, count in word_counts.items():\n","        # Join characters with a space and add the end-of-word marker\n","        bpe_word = ' '.join(list(word)) + ' </w>'\n","        vocab[bpe_word] = count\n","        \n","    return vocab\n","\n","# --- Let's test it with our example corpus ---\n","\n","corpus_text = \"low lower newest wider low low\"\n","initial_vocab = get_vocab(corpus_text)\n","\n","print(\"--- Initial Vocabulary ---\")\n","# Use a nicer format for printing the dictionary\n","for word, count in initial_vocab.items():\n","    print(f\"'{word}': {count}\")\n"]},{"cell_type":"markdown","id":"b90b62de","metadata":{"papermill":{"duration":0.003441,"end_time":"2025-10-25T10:14:19.990032","exception":false,"start_time":"2025-10-25T10:14:19.986591","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 2: Finding the Most Frequent Pair</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        This is the \"counting\" step of the BPE loop. At each iteration, we need to scan our entire current vocabulary and count the occurrences of all adjacent pairs of symbols.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        For example, in the vocabulary <code style=\"color: #e2e8f0;\">{'l o w </w>': 3, 'l o w e r </w>': 1}</code>, the pair <span class=\"bpe-kbd-dark\">l o</span> appears in both keys. Its total count would be its count in the first word (3) plus its count in the second word (1), for a total of <strong>4</strong>.\n","    </p>\n","     <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This function will iterate through every word-tokenization in our vocabulary, find all adjacent pairs within it, and sum up their counts, weighted by the frequency of the word itself. The function will return a dictionary of pairs and their total frequencies.\n","    </p>\n","</div>\n"]},{"cell_type":"code","execution_count":2,"id":"a7963758","metadata":{"execution":{"iopub.execute_input":"2025-10-25T10:14:19.999111Z","iopub.status.busy":"2025-10-25T10:14:19.998722Z","iopub.status.idle":"2025-10-25T10:14:20.007683Z","shell.execute_reply":"2025-10-25T10:14:20.006062Z"},"papermill":{"duration":0.015849,"end_time":"2025-10-25T10:14:20.009589","exception":false,"start_time":"2025-10-25T10:14:19.99374","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Pair Frequencies (Sorted) ---\n","('l', 'o'): 4\n","('o', 'w'): 4\n","('w', '</w>'): 3\n","('w', 'e'): 2\n","('e', 'r'): 2\n","('r', '</w>'): 2\n","('n', 'e'): 1\n","('e', 'w'): 1\n","('e', 's'): 1\n","('s', 't'): 1\n","('t', '</w>'): 1\n","('w', 'i'): 1\n","('i', 'd'): 1\n","('d', 'e'): 1\n"]}],"source":["import collections\n","\n","# We assume 'initial_vocab' exists from the previous cell.\n","# If running this cell alone, uncomment the line below:\n","# initial_vocab = {'l o w </w>': 3, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n","\n","def get_pair_stats(vocab: dict) -> collections.Counter:\n","    \"\"\"\n","    Counts the frequency of each adjacent pair of symbols in the vocabulary.\n","\n","    Args:\n","        vocab (dict): The current vocabulary mapping token sequences to their counts.\n","\n","    Returns:\n","        collections.Counter: A Counter object mapping pairs (tuples) to their frequencies.\n","    \"\"\"\n","    pair_counts = collections.Counter()\n","    \n","    for word, count in vocab.items():\n","        symbols = word.split()\n","        \n","        # Iterate through symbols to find adjacent pairs\n","        for i in range(len(symbols) - 1):\n","            pair = (symbols[i], symbols[i+1])\n","            # Increment the pair's count by the frequency of the word it appeared in\n","            pair_counts[pair] += count\n","            \n","    return pair_counts\n","\n","# --- Let's test it with our initial_vocab from Step 1 ---\n","\n","pair_stats = get_pair_stats(initial_vocab)\n","\n","print(\"--- Pair Frequencies (Sorted) ---\")\n","# .most_common() conveniently sorts them from most to least common\n","for pair, count in pair_stats.most_common():\n","    print(f\"{pair}: {count}\")\n"]},{"cell_type":"markdown","id":"67219daa","metadata":{"papermill":{"duration":0.003611,"end_time":"2025-10-25T10:14:20.017242","exception":false,"start_time":"2025-10-25T10:14:20.013631","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 3: Merging the Best Pair</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        Once we've identified the most frequent pair using our <code>get_pair_stats</code> function, the next logical step is to merge that pair into a single, new token. This is the core \"encoding\" step.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This operation involves creating a new vocabulary by iterating through the old one. For each word in the vocabulary, we find all occurrences of the target pair (e.g., <span class=\"bpe-kbd-dark\">l o</span>) and replace them with the newly merged token (e.g., <span class=\"bpe-kbd-dark\">lo</span>). The space between the original pair is removed, signifying they are now one unit.\n","    </p>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">Example</h4>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px; color: #a0aec0;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Best Pair to Merge:</strong> <code>('l', 'o')</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Old Word Tokenization:</strong> <span class=\"bpe-kbd-dark\">l o w </w></span></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Word Tokenization:</strong> <span class=\"bpe-kbd-dark\">lo w </w></span></li>\n","        </ul>\n","    </div>\n","</div>\n"]},{"cell_type":"code","execution_count":3,"id":"decd79d6","metadata":{"execution":{"iopub.execute_input":"2025-10-25T10:14:20.026194Z","iopub.status.busy":"2025-10-25T10:14:20.025845Z","iopub.status.idle":"2025-10-25T10:14:20.033564Z","shell.execute_reply":"2025-10-25T10:14:20.032464Z"},"papermill":{"duration":0.01424,"end_time":"2025-10-25T10:14:20.035149","exception":false,"start_time":"2025-10-25T10:14:20.020909","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Vocabulary after merging ('l', 'o') ---\n","'lo w </w>': 3\n","'lo w e r </w>': 1\n","'n e w e s t </w>': 1\n","'w i d e r </w>': 1\n"]}],"source":["import re\n","\n","def merge_vocab(pair: tuple, v_in: dict) -> dict:\n","    \"\"\"\n","    Merges the most frequent pair in the vocabulary.\n","\n","    Args:\n","        pair (tuple): The pair of symbols to merge (e.g., ('l', 'o')).\n","        v_in (dict): The vocabulary before the merge.\n","\n","    Returns:\n","        dict: The vocabulary after the merge.\n","    \"\"\"\n","    v_out = {}\n","    \n","    # The pair as a string with a space, for regex replacement.\n","    # We use re.escape to handle tokens that might contain special regex characters.\n","    bigram = re.escape(' '.join(pair))\n","    \n","    # The merged pair as a single token string\n","    p = ''.join(pair)\n","    \n","    # Compile a regex to find the bigram. The (?<!\\S) and (?!\\S) are negative\n","    # lookarounds that ensure we are matching the whole token, not a substring of another token.\n","    # For example, it prevents merging 's' 't' inside 'est'.\n","    regex = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    \n","    for word in v_in:\n","        # Replace the pair with the merged token in the word representation\n","        w_out = regex.sub(p, word)\n","        v_out[w_out] = v_in[word]\n","        \n","    return v_out\n","\n","# --- Let's test it with our previous results ---\n","\n","# From our pair_stats, the best pair is ('l', 'o') with a count of 4.\n","# (If there's a tie, we just pick one. 'o w' also had a count of 4).\n","best_pair = ('l', 'o')\n","\n","# We use 'initial_vocab' from Step 1.\n","# If running this cell alone, uncomment the line below:\n","# initial_vocab = {'l o w </w>': 3, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n","\n","# Perform the merge\n","merged_vocab = merge_vocab(best_pair, initial_vocab)\n","\n","print(f\"--- Vocabulary after merging {best_pair} ---\")\n","for word, count in merged_vocab.items():\n","    print(f\"'{word}': {count}\")\n"]},{"cell_type":"markdown","id":"5e4d8bc6","metadata":{"papermill":{"duration":0.003467,"end_time":"2025-10-25T10:14:20.042783","exception":false,"start_time":"2025-10-25T10:14:20.039316","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 4: Putting It All Together - The Training Loop</h3>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        This is where we orchestrate the entire BPE training process. We'll create a main function that runs a loop for a specified number of merges (<code>num_merges</code>). This hyperparameter controls the size of our final vocabulary.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">In each iteration of the loop, the algorithm will perform the following actions:</p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Call <code>get_pair_stats</code> on the current vocabulary to count all adjacent pairs.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Identify the most frequent pair from the statistics. If no pairs are left, the training stops early.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Record this \"best pair\" as a new <strong>merge rule</strong>. The ordered list of these rules is the primary output of our BPE training.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Call <code>merge_vocab</code> to update the vocabulary by merging the best pair into a single token.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Repeat this process until the desired number of merges is complete.</li>\n","    </ol>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        The two final outputs will be the final state of the vocabulary and, more importantly, the ordered list of <strong>merge rules</strong>. This list is what we need to tokenize new, unseen text.\n","    </p>\n","</div>\n"]},{"cell_type":"code","execution_count":4,"id":"9ddaf9c7","metadata":{"execution":{"iopub.execute_input":"2025-10-25T10:14:20.052014Z","iopub.status.busy":"2025-10-25T10:14:20.051694Z","iopub.status.idle":"2025-10-25T10:14:20.067362Z","shell.execute_reply":"2025-10-25T10:14:20.066031Z"},"papermill":{"duration":0.022328,"end_time":"2025-10-25T10:14:20.068918","exception":false,"start_time":"2025-10-25T10:14:20.04659","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Initial Vocabulary ---\n","{'l o w </w>': 3, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n","--------------------------------------------------\n","Merge 1/10: Merged ('l', 'o') -> lo\n","  Current Vocab: ['lo w </w>', 'lo w e r </w>', 'n e w e s t </w>', 'w i d e r </w>']\n","\n","Merge 2/10: Merged ('lo', 'w') -> low\n","  Current Vocab: ['low </w>', 'low e r </w>', 'n e w e s t </w>', 'w i d e r </w>']\n","\n","Merge 3/10: Merged ('low', '</w>') -> low</w>\n","  Current Vocab: ['low</w>', 'low e r </w>', 'n e w e s t </w>', 'w i d e r </w>']\n","\n","Merge 4/10: Merged ('e', 'r') -> er\n","  Current Vocab: ['low</w>', 'low er </w>', 'n e w e s t </w>', 'w i d er </w>']\n","\n","Merge 5/10: Merged ('er', '</w>') -> er</w>\n","  Current Vocab: ['low</w>', 'low er</w>', 'n e w e s t </w>', 'w i d er</w>']\n","\n","Merge 6/10: Merged ('low', 'er</w>') -> lower</w>\n","  Current Vocab: ['low</w>', 'lower</w>', 'n e w e s t </w>', 'w i d er</w>']\n","\n","Merge 7/10: Merged ('n', 'e') -> ne\n","  Current Vocab: ['low</w>', 'lower</w>', 'ne w e s t </w>', 'w i d er</w>']\n","\n","Merge 8/10: Merged ('ne', 'w') -> new\n","  Current Vocab: ['low</w>', 'lower</w>', 'new e s t </w>', 'w i d er</w>']\n","\n","Merge 9/10: Merged ('new', 'e') -> newe\n","  Current Vocab: ['low</w>', 'lower</w>', 'newe s t </w>', 'w i d er</w>']\n","\n","Merge 10/10: Merged ('newe', 's') -> newes\n","  Current Vocab: ['low</w>', 'lower</w>', 'newes t </w>', 'w i d er</w>']\n","\n","\n","========================= TRAINING COMPLETE =========================\n","\n","--- Final Vocabulary State ---\n","'low</w>': 3\n","'lower</w>': 1\n","'newes t </w>': 1\n","'w i d er</w>': 1\n","\n","--- Learned Merge Rules (in order) ---\n","1: ('l', 'o')\n","2: ('lo', 'w')\n","3: ('low', '</w>')\n","4: ('e', 'r')\n","5: ('er', '</w>')\n","6: ('low', 'er</w>')\n","7: ('n', 'e')\n","8: ('ne', 'w')\n","9: ('new', 'e')\n","10: ('newe', 's')\n"]}],"source":["import collections\n","import re\n","\n","# We need to include all our helper functions in this cell to make it self-contained\n","# and runnable in the Kaggle notebook.\n","\n","def get_vocab(corpus: str) -> dict:\n","    \"\"\"Creates an initial vocabulary from a raw text corpus.\"\"\"\n","    words = corpus.strip().split()\n","    word_counts = collections.Counter(words)\n","    vocab = {' '.join(list(word)) + ' </w>': count for word, count in word_counts.items()}\n","    return vocab\n","\n","def get_pair_stats(vocab: dict) -> collections.Counter:\n","    \"\"\"Counts the frequency of each adjacent pair of symbols.\"\"\"\n","    pair_counts = collections.Counter()\n","    for word, count in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols) - 1):\n","            pair_counts[(symbols[i], symbols[i+1])] += count\n","    return pair_counts\n","\n","def merge_vocab(pair: tuple, v_in: dict) -> dict:\n","    \"\"\"Merges a specific pair in the vocabulary.\"\"\"\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair))\n","    p = ''.join(pair)\n","    regex = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    for word in v_in:\n","        w_out = regex.sub(p, word)\n","        v_out[w_out] = v_in[word]\n","    return v_out\n","\n","# --- The Main BPE Training Function ---\n","\n","def train_bpe(corpus: str, num_merges: int, verbose: bool = True):\n","    \"\"\"\n","    Trains a BPE model on a corpus.\n","\n","    Args:\n","        corpus (str): The training text.\n","        num_merges (int): The number of merge operations to perform.\n","        verbose (bool): If True, prints the progress of each merge.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - The final vocabulary after all merges.\n","            - A list of the merge rules in the order they were learned.\n","    \"\"\"\n","    # 1. Initialize vocabulary from the corpus\n","    vocab = get_vocab(corpus)\n","    \n","    # This will store our ordered merge rules\n","    merge_rules = []\n","    \n","    if verbose:\n","        print(\"--- Initial Vocabulary ---\")\n","        print(vocab)\n","        print(\"-\" * 50)\n","    \n","    for i in range(num_merges):\n","        # 2. Get pair statistics from the current vocabulary\n","        pair_stats = get_pair_stats(vocab)\n","        \n","        # If there are no more pairs to merge, stop\n","        if not pair_stats:\n","            if verbose: print(\"No more pairs to merge. Stopping early.\")\n","            break\n","        \n","        # 3. Find the most frequent pair\n","        best_pair = max(pair_stats, key=pair_stats.get)\n","        merge_rules.append(best_pair)\n","        \n","        # 4. Merge the best pair in the vocabulary\n","        vocab = merge_vocab(best_pair, vocab)\n","        \n","        if verbose:\n","            print(f\"Merge {i+1}/{num_merges}: Merged {best_pair} -> {''.join(best_pair)}\")\n","            print(f\"  Current Vocab: {list(vocab.keys())}\\n\")\n","            \n","    return vocab, merge_rules\n","\n","# --- Let's run the full training loop on our corpus ---\n","\n","corpus_text = \"low lower newest wider low low\"\n","num_merges_to_perform = 10\n","\n","final_vocab, learned_rules = train_bpe(corpus_text, num_merges_to_perform)\n","\n","print(\"\\n\" + \"=\"*25 + \" TRAINING COMPLETE \" + \"=\"*25)\n","print(\"\\n--- Final Vocabulary State ---\")\n","for word, count in final_vocab.items():\n","    print(f\"'{word}': {count}\")\n","\n","print(\"\\n--- Learned Merge Rules (in order) ---\")\n","for i, rule in enumerate(learned_rules):\n","    print(f\"{i+1}: {rule}\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":6.183118,"end_time":"2025-10-25T10:14:20.498194","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-25T10:14:14.315076","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}