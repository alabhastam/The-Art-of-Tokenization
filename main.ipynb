{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/the-art-of-tokenization?scriptVersionId=270368476\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"db78048d","metadata":{"papermill":{"duration":0.002268,"end_time":"2025-10-23T20:05:05.454593","exception":false,"start_time":"2025-10-23T20:05:05.452325","status":"completed"},"tags":[]},"source":["# The Art of Tokenization, Part 1: Byte Pair Encoding"]},{"cell_type":"markdown","id":"ed97b896","metadata":{"papermill":{"duration":0.00147,"end_time":"2025-10-23T20:05:05.458169","exception":false,"start_time":"2025-10-23T20:05:05.456699","status":"completed"},"tags":[]},"source":["Welcome to this series on the art and science of tokenization! In Natural Language Processing (NLP), before a powerful model like GPT or BERT can understand human language, we must first break that text down into pieces it can recognize. These pieces are called tokens. The process of creating them is tokenization, and it’s one of the most fundamental steps in any NLP pipeline.\n","\n","This series will explore the different strategies for tokenization. We’ll start with one of the most influential methods in the modern NLP landscape: Byte Pair Encoding (BPE)."]},{"cell_type":"markdown","id":"072a3657","metadata":{"papermill":{"duration":0.001345,"end_time":"2025-10-23T20:05:05.461153","exception":false,"start_time":"2025-10-23T20:05:05.459808","status":"completed"},"tags":[]},"source":["# What is BPE? The “Happy Medium” of Tokenization\n","At its core, Byte Pair Encoding (BPE) is a subword tokenization algorithm. Instead of forcing us to choose between whole words or individual characters, it finds a “happy medium.”\n"]},{"cell_type":"markdown","id":"6c9efd4b","metadata":{"papermill":{"duration":0.001389,"end_time":"2025-10-23T20:05:05.464079","exception":false,"start_time":"2025-10-23T20:05:05.46269","status":"completed"},"tags":[]},"source":["Imagine trying to create a dictionary for a language model:\n","* Option A: Word Dictionary. You include every single word (\"cat\", \"run\", \"photosynthesis\", \"antidisestablishmentarianism\").\n","\n","* Problem: The dictionary becomes enormous. What about new words (“de-platforming”), slang (“yeet”), or simple typos (“helllo”)? They are all “Out-of-Vocabulary” (OOV) and become a meaningless <UNK> (unknown) token.\n","\n","* Option B: Character Dictionary. You only include characters ('a', 'b', 'c', '!').\n","\n","* Problem: No more OOV issues, but the text sequences become incredibly long. The sentence “Hello world” is now 11 tokens ('H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'). This makes it computationally expensive and harder for the model to find meaning."]},{"cell_type":"markdown","id":"cf6d0a57","metadata":{"papermill":{"duration":0.001752,"end_time":"2025-10-23T20:05:05.467356","exception":false,"start_time":"2025-10-23T20:05:05.465604","status":"completed"},"tags":[]},"source":["This way, a common word like \"where\" can be a single token, while a rare, complex word like \"retrofitting\" can be broken down into meaningful pieces like [\"retro\", \"fit\", \"ting\"]. Crucially, no word is ever “unknown.” Any new word can be built from these subword pieces."]},{"cell_type":"markdown","id":"05c77e92","metadata":{"papermill":{"duration":0.001366,"end_time":"2025-10-23T20:05:05.47039","exception":false,"start_time":"2025-10-23T20:05:05.469024","status":"completed"},"tags":[]},"source":["# How the BPE Algorithm Works (Conceptually)\n","BPE was originally a data compression algorithm. Its adaptation for NLP is based on a simple, greedy, and iterative idea:\n","> Core Logic: Continuously find the most common pair of adjacent tokens in your text and merge them into a single, new token.\n"]},{"cell_type":"markdown","id":"a4ccc804","metadata":{"papermill":{"duration":0.001334,"end_time":"2025-10-23T20:05:05.473279","exception":false,"start_time":"2025-10-23T20:05:05.471945","status":"completed"},"tags":[]},"source":["Let’s trace the process with a tiny imaginary corpus.\n","\n","* Corpus: (low, low, low, lower, newest, wider)\n","* step 0: First, we break every word down into its basic characters. We also add a special symbol, like </w>, to mark the end of a word. This is important to distinguish between er inside a word (like in “newest”) and er at the end of a word (like in “lower”). Our initial “tokens” are just characters: {l, o, w, e, r, n, s, t, i, d, </w>}.\n","* Step 1: First MergeThe algorithm scans the entire corpus and counts the frequency of every adjacent pair of tokens.\n","Let’s say it finds that the pair e followed by r (e r) is the most common combination (appearing in lower and wider)."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":5.583666,"end_time":"2025-10-23T20:05:05.893214","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-23T20:05:00.309548","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}