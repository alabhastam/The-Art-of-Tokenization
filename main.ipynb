{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/the-art-of-tokenization?scriptVersionId=271099105\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"b327630f","metadata":{"papermill":{"duration":0.005939,"end_time":"2025-10-26T19:40:55.522986","exception":false,"start_time":"2025-10-26T19:40:55.517047","status":"completed"},"tags":[]},"source":["# The Art of Tokenization, Part 1: Byte Pair Encoding"]},{"cell_type":"markdown","id":"11eb76c4","metadata":{"papermill":{"duration":0.004224,"end_time":"2025-10-26T19:40:55.532024","exception":false,"start_time":"2025-10-26T19:40:55.5278","status":"completed"},"tags":[]},"source":["Welcome to this series on the art and science of tokenization! In Natural Language Processing (NLP), before a powerful model like GPT or BERT can understand human language, we must first break that text down into pieces it can recognize. These pieces are called tokens. The process of creating them is tokenization, and it’s one of the most fundamental steps in any NLP pipeline.\n","\n","This series will explore the different strategies for tokenization. We’ll start with one of the most influential methods in the modern NLP landscape: Byte Pair Encoding (BPE)."]},{"cell_type":"markdown","id":"4adbd12d","metadata":{"papermill":{"duration":0.004064,"end_time":"2025-10-26T19:40:55.54059","exception":false,"start_time":"2025-10-26T19:40:55.536526","status":"completed"},"tags":[]},"source":["# What is BPE? The “Happy Medium” of Tokenization\n","At its core, Byte Pair Encoding (BPE) is a subword tokenization algorithm. Instead of forcing us to choose between whole words or individual characters, it finds a “happy medium.”\n"]},{"cell_type":"markdown","id":"078b8bfb","metadata":{"papermill":{"duration":0.00407,"end_time":"2025-10-26T19:40:55.54905","exception":false,"start_time":"2025-10-26T19:40:55.54498","status":"completed"},"tags":[]},"source":["Imagine trying to create a dictionary for a language model:\n","* Option A: Word Dictionary. You include every single word (\"cat\", \"run\", \"photosynthesis\", \"antidisestablishmentarianism\").\n","\n","* Problem: The dictionary becomes enormous. What about new words (“de-platforming”), slang (“yeet”), or simple typos (“helllo”)? They are all “Out-of-Vocabulary” (OOV) and become a meaningless <UNK> (unknown) token.\n","\n","* Option B: Character Dictionary. You only include characters ('a', 'b', 'c', '!').\n","\n","* Problem: No more OOV issues, but the text sequences become incredibly long. The sentence “Hello world” is now 11 tokens ('H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'). This makes it computationally expensive and harder for the model to find meaning."]},{"cell_type":"markdown","id":"3fadbda1","metadata":{"papermill":{"duration":0.004041,"end_time":"2025-10-26T19:40:55.557427","exception":false,"start_time":"2025-10-26T19:40:55.553386","status":"completed"},"tags":[]},"source":["This way, a common word like \"where\" can be a single token, while a rare, complex word like \"retrofitting\" can be broken down into meaningful pieces like [\"retro\", \"fit\", \"ting\"]. Crucially, no word is ever “unknown.” Any new word can be built from these subword pieces."]},{"cell_type":"markdown","id":"d77a3e36","metadata":{"papermill":{"duration":0.004243,"end_time":"2025-10-26T19:40:55.565923","exception":false,"start_time":"2025-10-26T19:40:55.56168","status":"completed"},"tags":[]},"source":["# How the BPE Algorithm Works (Conceptually)\n","BPE was originally a data compression algorithm. Its adaptation for NLP is based on a simple, greedy, and iterative idea:\n","> Core Logic: Continuously find the most common pair of adjacent tokens in your text and merge them into a single, new token.\n"]},{"cell_type":"markdown","id":"1ad3dce1","metadata":{"papermill":{"duration":0.004013,"end_time":"2025-10-26T19:40:55.57447","exception":false,"start_time":"2025-10-26T19:40:55.570457","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">How the BPE Algorithm Works (Conceptually)</h3>\n","    <p style=\"color: #a0aec0;\">BPE is based on a simple, greedy, and iterative idea: continuously find the most common pair of adjacent tokens and merge them. Let's trace the process.</p>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n","<div style=\"margin-top: 25px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 0: Initialize</h4>\n","        <p style=\"margin: 0;\">\n","            First, we break every word down into its basic characters. We also add a special symbol, like <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>, to mark the end of a word. This is important to distinguish between <span class=\"bpe-kbd-dark\">er</span> inside a word (like in “newest”) and <span class=\"bpe-kbd-dark\">er</span> at the end of a word (like in “lower”).\n","        </p>\n","        <p style=\"margin-top: 10px;\">\n","            Our initial “tokens” are just characters: <span class=\"bpe-kbd-dark\">l</span> <span class=\"bpe-kbd-dark\">o</span> <span class=\"bpe-kbd-dark\">w</span> <span class=\"bpe-kbd-dark\">e</span> <span class=\"bpe-kbd-dark\">r</span> <span class=\"bpe-kbd-dark\">n</span> <span class=\"bpe-kbd-dark\">s</span> <span class=\"bpe-kbd-dark\">t</span> <span class=\"bpe-kbd-dark\">i</span> <span class=\"bpe-kbd-dark\">d</span> <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>.\n","        </p>\n","    </div>\n","\n","<div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 1: First Merge</h4>\n","        <p>The algorithm scans the entire corpus and counts the frequency of every adjacent pair of tokens.</p>\n","        <p>Let’s say it finds that the pair <span class=\"bpe-kbd-dark\">e</span> followed by <span class=\"bpe-kbd-dark\">r</span> (i.e., <span class=\"bpe-kbd-dark\">e r</span>) is the most common combination (appearing in <em>lower</em> and <em>wider</em>).</p>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Action:</strong> It merges <span class=\"bpe-kbd-dark\">e</span> and <span class=\"bpe-kbd-dark\">r</span> to create a new token, <span class=\"bpe-kbd-dark\">er</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Vocabulary:</strong> Our vocabulary now includes <span class=\"bpe-kbd-dark\">er</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Corpus Update:</strong> All instances of <span class=\"bpe-kbd-dark\">e r</span> are replaced. So, <span class=\"bpe-kbd-dark\">l o w e r &lt;/w&gt;</span> becomes <span class=\"bpe-kbd-dark\">l o w er &lt;/w&gt;</span>.</li>\n","        </ul>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 2: Second Merge</h4>\n","        <p>The process repeats. The algorithm scans the <em>updated</em> corpus. Perhaps it now finds that <span class=\"bpe-kbd-dark\">er</span> followed by <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span> (i.e., <span class=\"bpe-kbd-dark\">er &lt;/w&gt;</span>) is the most frequent pair.</p>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Action:</strong> It merges <span class=\"bpe-kbd-dark\">er</span> and <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span> to create a new token, <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Vocabulary:</strong> Our vocabulary now includes <span class=\"bpe-kbd-dark\">er</span> and <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Corpus Update:</strong> <span class=\"bpe-kbd-dark\">l o w er &lt;/w&gt;</span> becomes <span class=\"bpe-kbd-dark\">l o w er&lt;/w&gt;</span>.</li>\n","        </ul>\n","    </div>\n","    \n"," <div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 3: Keep Going…</h4>\n","        <p>Next, maybe <span class=\"bpe-kbd-dark\">l</span> followed by <span class=\"bpe-kbd-dark\">o</span> (<span class=\"bpe-kbd-dark\">l o</span>) is the most common. They get merged into <span class=\"bpe-kbd-dark\">lo</span>. Then <span class=\"bpe-kbd-dark\">lo</span> and <span class=\"bpe-kbd-dark\">w</span> get merged into <span class=\"bpe-kbd-dark\">low</span>.</p>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">The End Result</h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This process is repeated for a predetermined number of merges (e.g., <strong>30,000 times</strong>). The final vocabulary consists of the initial characters plus all the new tokens created during the merges. This gives us a ranked list of merge rules that we can use to tokenize any new text.</p>\n","    </div>\n","</div>\n"]},{"cell_type":"markdown","id":"0e62234d","metadata":{"papermill":{"duration":0.004017,"end_time":"2025-10-26T19:40:55.582805","exception":false,"start_time":"2025-10-26T19:40:55.578788","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","  <h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">\n","    Pros and Cons of BPE\n","  </h3>\n","  <p style=\"color: #a0aec0;\">\n","    BPE is widely used for a reason, but it’s not without its trade‑offs.\n","  </p>\n","\n","  <style>\n","    .bpe-kbd-dark {\n","      background-color: #1a202c;\n","      border: 1px solid #4a5568;\n","      border-bottom: 2px solid #718096;\n","      border-radius: 4px;\n","      padding: 3px 6px;\n","      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","      font-size: 0.9em;\n","      color: #cbd5e1;\n","      white-space: nowrap;\n","    }\n","  </style>\n","\n","  <!-- ✅ PROS SECTION -->\n","  <div style=\"margin-top: 25px;\">\n","    <h4 style=\"color: #68d391; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      Advantages (Pros) 👍\n","    </h4>\n","    <ul style=\"list-style-type: '✔️'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Eliminates Out‑of‑Vocabulary (OOV) Words:</strong> Any new or rare word can be decomposed into a sequence of known subword tokens. The model never has to deal with a completely \n","        <span class=\"bpe-kbd-dark\">&lt;UNK&gt;</span> token.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Controllable Vocabulary Size:</strong> The final vocabulary size is a hyperparameter — the number of merges. You can balance model size and expressiveness.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Efficiency:</strong> Common words stay as single tokens, making sequences shorter and inference faster. Rare words are decomposed smartly, making better use of vocab space.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Captures Morphology:</strong> Learns meaningful parts like prefixes (<span class=\"bpe-kbd-dark\">un-</span>), suffixes (<span class=\"bpe-kbd-dark\">-ing</span>), and stems, helping generalization. If it learns \n","        <span class=\"bpe-kbd-dark\">un</span>, it can recognize \n","        <em>unhappy</em>, <em>unclear</em>, <em>unbelievable</em>.\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- ❌ CONS SECTION -->\n","  <div style=\"margin-top: 35px; border-top: 1px dashed #4a5568; padding-top: 25px;\">\n","    <h4 style=\"color: #f56565; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      Disadvantages (Cons) 👎\n","    </h4>\n","    <ul style=\"list-style-type: '⚠️'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Greedy Approach:</strong> BPE makes local (greedy) merges — not necessarily globally optimal. A different merge order might yield a better vocabulary.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Sensitive to Training Data:</strong> Merge rules depend entirely on the training corpus. A BPE trained on Wikipedia would tokenize Twitter slang or biomedical text poorly.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>The “Byte” Misnomer:</strong> Most modern BPE operates on characters, not raw bytes. True byte‑level BPE (used in GPT‑2) works directly on UTF‑8 bytes and is more robust — handling any text, emoji, or noise.\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- 🌍 CONTEXT SECTION -->\n","  <div style=\"margin-top: 35px; border-top: 1px dashed #4a5568; padding-top: 25px;\">\n","    <h4 style=\"color: #b794f4; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      BPE’s Place in the Tokenization World\n","    </h4>\n","    <p style=\"color: #a0aec0;\">\n","      BPE is a cornerstone method for subword tokenization, but it’s not alone. Its relatives include:\n","    </p>\n","    <ul style=\"list-style-type: '🔹'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>WordPiece</strong> (<span class=\"bpe-kbd-dark\">BERT</span>, <span class=\"bpe-kbd-dark\">DistilBERT</span>): Similar to BPE but merges based on the merge that <em>maximizes the likelihood</em> of the data instead of raw frequency.\n","      </li>\n","      <li>\n","        <strong>Unigram Language Model</strong> (<span class=\"bpe-kbd-dark\">T5</span>, <span class=\"bpe-kbd-dark\">ALBERT</span>, <span class=\"bpe-kbd-dark\">XLNet</span>): A probabilistic approach that prunes subwords contributing least to total corpus probability — can produce multiple valid tokenizations per word (useful as regularization).\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- ⚡ FINAL NOTE -->\n","  <div style=\"margin-top: 30px; background-color: #1a202c; border-left: 5px solid #3182ce; padding: 15px; border-radius: 6px;\">\n","    <p style=\"margin: 0; color: #a0aec0;\">\n","      Now that we understand the theory, let’s roll up our sleeves and build a \n","      <strong style=\"color:#63b3ed;\">BPE tokenizer</strong> from scratch 🔧\n","    </p>\n","  </div>\n","</div>\n"]},{"cell_type":"markdown","id":"fc959984","metadata":{"papermill":{"duration":0.004278,"end_time":"2025-10-26T19:40:55.591424","exception":false,"start_time":"2025-10-26T19:40:55.587146","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 1: Preparing the Training Corpus</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        Before we can start merging pairs, we need a corpus of text in the right format. The BPE algorithm operates on a vocabulary built from words and their frequencies. So, our first step is to:\n","    </p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Take a raw text corpus.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Split it into individual words and count the frequency of each unique word.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Represent each word as a sequence of characters, separated by spaces, and add a special end-of-word marker, like <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>.</li>\n","    </ol>\n","     <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This marker is crucial as it allows the algorithm to distinguish between a subword found inside a larger word (like <span class=\"bpe-kbd-dark\">er</span> in \"newer\") and a subword that forms the end of a word (like <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span> in \"lower\"). Our goal is to transform a block of text into a dictionary where keys are the character-split words and values are their frequencies.\n","    </p>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">Example</h4>\n","         <p style=\"margin: 0; color: #a0aec0;\">\n","            <strong>Input Corpus:</strong> <code>\"low lower newest wider low\"</code><br>\n","            <strong>Becomes:</strong> <code>{'l o w </w>': 2, 'l o w e r </w>': 1, ...}</code>\n","        </p>\n","    </div>\n","</div>\n"]},{"cell_type":"code","execution_count":1,"id":"86e97fbd","metadata":{"execution":{"iopub.execute_input":"2025-10-26T19:40:55.601281Z","iopub.status.busy":"2025-10-26T19:40:55.600949Z","iopub.status.idle":"2025-10-26T19:40:55.611164Z","shell.execute_reply":"2025-10-26T19:40:55.610277Z"},"papermill":{"duration":0.016759,"end_time":"2025-10-26T19:40:55.612492","exception":false,"start_time":"2025-10-26T19:40:55.595733","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Initial Vocabulary ---\n","'l o w </w>': 3\n","'l o w e r </w>': 1\n","'n e w e s t </w>': 1\n","'w i d e r </w>': 1\n"]}],"source":["import collections\n","\n","def get_vocab(corpus: str) -> dict:\n","    \"\"\"\n","    Takes a raw text corpus and creates an initial vocabulary.\n","    \n","    The vocabulary maps each word (split into characters and with an end-of-word marker)\n","    to its frequency in the corpus.\n","\n","    Args:\n","        corpus (str): A string containing the training text.\n","\n","    Returns:\n","        dict: A dictionary where keys are space-separated characters of words\n","              (e.g., 'l o w </w>') and values are their counts.\n","    \"\"\"\n","    # Split the corpus into words and count their frequencies\n","    words = corpus.strip().split()\n","    word_counts = collections.Counter(words)\n","    \n","    # Initialize the vocabulary dictionary\n","    vocab = {}\n","    \n","    # For each word and its count, format it for BPE\n","    for word, count in word_counts.items():\n","        # Join characters with a space and add the end-of-word marker\n","        bpe_word = ' '.join(list(word)) + ' </w>'\n","        vocab[bpe_word] = count\n","        \n","    return vocab\n","\n","# --- Let's test it with our example corpus ---\n","\n","corpus_text = \"low lower newest wider low low\"\n","initial_vocab = get_vocab(corpus_text)\n","\n","print(\"--- Initial Vocabulary ---\")\n","# Use a nicer format for printing the dictionary\n","for word, count in initial_vocab.items():\n","    print(f\"'{word}': {count}\")\n"]},{"cell_type":"markdown","id":"ffb5cad8","metadata":{"papermill":{"duration":0.004015,"end_time":"2025-10-26T19:40:55.62107","exception":false,"start_time":"2025-10-26T19:40:55.617055","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 2: Finding the Most Frequent Pair</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        This is the \"counting\" step of the BPE loop. At each iteration, we need to scan our entire current vocabulary and count the occurrences of all adjacent pairs of symbols.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        For example, in the vocabulary <code style=\"color: #e2e8f0;\">{'l o w </w>': 3, 'l o w e r </w>': 1}</code>, the pair <span class=\"bpe-kbd-dark\">l o</span> appears in both keys. Its total count would be its count in the first word (3) plus its count in the second word (1), for a total of <strong>4</strong>.\n","    </p>\n","     <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This function will iterate through every word-tokenization in our vocabulary, find all adjacent pairs within it, and sum up their counts, weighted by the frequency of the word itself. The function will return a dictionary of pairs and their total frequencies.\n","    </p>\n","</div>\n"]},{"cell_type":"code","execution_count":2,"id":"d431ccaf","metadata":{"execution":{"iopub.execute_input":"2025-10-26T19:40:55.630876Z","iopub.status.busy":"2025-10-26T19:40:55.630596Z","iopub.status.idle":"2025-10-26T19:40:55.637312Z","shell.execute_reply":"2025-10-26T19:40:55.63631Z"},"papermill":{"duration":0.013324,"end_time":"2025-10-26T19:40:55.638688","exception":false,"start_time":"2025-10-26T19:40:55.625364","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Pair Frequencies (Sorted) ---\n","('l', 'o'): 4\n","('o', 'w'): 4\n","('w', '</w>'): 3\n","('w', 'e'): 2\n","('e', 'r'): 2\n","('r', '</w>'): 2\n","('n', 'e'): 1\n","('e', 'w'): 1\n","('e', 's'): 1\n","('s', 't'): 1\n","('t', '</w>'): 1\n","('w', 'i'): 1\n","('i', 'd'): 1\n","('d', 'e'): 1\n"]}],"source":["import collections\n","\n","# We assume 'initial_vocab' exists from the previous cell.\n","# If running this cell alone, uncomment the line below:\n","# initial_vocab = {'l o w </w>': 3, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n","\n","def get_pair_stats(vocab: dict) -> collections.Counter:\n","    \"\"\"\n","    Counts the frequency of each adjacent pair of symbols in the vocabulary.\n","\n","    Args:\n","        vocab (dict): The current vocabulary mapping token sequences to their counts.\n","\n","    Returns:\n","        collections.Counter: A Counter object mapping pairs (tuples) to their frequencies.\n","    \"\"\"\n","    pair_counts = collections.Counter()\n","    \n","    for word, count in vocab.items():\n","        symbols = word.split()\n","        \n","        # Iterate through symbols to find adjacent pairs\n","        for i in range(len(symbols) - 1):\n","            pair = (symbols[i], symbols[i+1])\n","            # Increment the pair's count by the frequency of the word it appeared in\n","            pair_counts[pair] += count\n","            \n","    return pair_counts\n","\n","# --- Let's test it with our initial_vocab from Step 1 ---\n","\n","pair_stats = get_pair_stats(initial_vocab)\n","\n","print(\"--- Pair Frequencies (Sorted) ---\")\n","# .most_common() conveniently sorts them from most to least common\n","for pair, count in pair_stats.most_common():\n","    print(f\"{pair}: {count}\")\n"]},{"cell_type":"markdown","id":"7870c449","metadata":{"papermill":{"duration":0.004091,"end_time":"2025-10-26T19:40:55.647366","exception":false,"start_time":"2025-10-26T19:40:55.643275","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 3: Merging the Best Pair</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        Once we've identified the most frequent pair using our <code>get_pair_stats</code> function, the next logical step is to merge that pair into a single, new token. This is the core \"encoding\" step.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This operation involves creating a new vocabulary by iterating through the old one. For each word in the vocabulary, we find all occurrences of the target pair (e.g., <span class=\"bpe-kbd-dark\">l o</span>) and replace them with the newly merged token (e.g., <span class=\"bpe-kbd-dark\">lo</span>). The space between the original pair is removed, signifying they are now one unit.\n","    </p>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">Example</h4>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px; color: #a0aec0;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Best Pair to Merge:</strong> <code>('l', 'o')</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Old Word Tokenization:</strong> <span class=\"bpe-kbd-dark\">l o w </w></span></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Word Tokenization:</strong> <span class=\"bpe-kbd-dark\">lo w </w></span></li>\n","        </ul>\n","    </div>\n","</div>\n"]},{"cell_type":"code","execution_count":3,"id":"24f7a97c","metadata":{"execution":{"iopub.execute_input":"2025-10-26T19:40:55.658344Z","iopub.status.busy":"2025-10-26T19:40:55.658058Z","iopub.status.idle":"2025-10-26T19:40:55.665653Z","shell.execute_reply":"2025-10-26T19:40:55.664564Z"},"papermill":{"duration":0.014557,"end_time":"2025-10-26T19:40:55.667216","exception":false,"start_time":"2025-10-26T19:40:55.652659","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Vocabulary after merging ('l', 'o') ---\n","'lo w </w>': 3\n","'lo w e r </w>': 1\n","'n e w e s t </w>': 1\n","'w i d e r </w>': 1\n"]}],"source":["import re\n","\n","def merge_vocab(pair: tuple, v_in: dict) -> dict:\n","    \"\"\"\n","    Merges the most frequent pair in the vocabulary.\n","\n","    Args:\n","        pair (tuple): The pair of symbols to merge (e.g., ('l', 'o')).\n","        v_in (dict): The vocabulary before the merge.\n","\n","    Returns:\n","        dict: The vocabulary after the merge.\n","    \"\"\"\n","    v_out = {}\n","    \n","    # The pair as a string with a space, for regex replacement.\n","    # We use re.escape to handle tokens that might contain special regex characters.\n","    bigram = re.escape(' '.join(pair))\n","    \n","    # The merged pair as a single token string\n","    p = ''.join(pair)\n","    \n","    # Compile a regex to find the bigram. The (?<!\\S) and (?!\\S) are negative\n","    # lookarounds that ensure we are matching the whole token, not a substring of another token.\n","    # For example, it prevents merging 's' 't' inside 'est'.\n","    regex = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    \n","    for word in v_in:\n","        # Replace the pair with the merged token in the word representation\n","        w_out = regex.sub(p, word)\n","        v_out[w_out] = v_in[word]\n","        \n","    return v_out\n","\n","# --- Let's test it with our previous results ---\n","\n","# From our pair_stats, the best pair is ('l', 'o') with a count of 4.\n","# (If there's a tie, we just pick one. 'o w' also had a count of 4).\n","best_pair = ('l', 'o')\n","\n","# We use 'initial_vocab' from Step 1.\n","# If running this cell alone, uncomment the line below:\n","# initial_vocab = {'l o w </w>': 3, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n","\n","# Perform the merge\n","merged_vocab = merge_vocab(best_pair, initial_vocab)\n","\n","print(f\"--- Vocabulary after merging {best_pair} ---\")\n","for word, count in merged_vocab.items():\n","    print(f\"'{word}': {count}\")\n"]},{"cell_type":"markdown","id":"af9f1cdb","metadata":{"papermill":{"duration":0.004322,"end_time":"2025-10-26T19:40:55.676251","exception":false,"start_time":"2025-10-26T19:40:55.671929","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 4: Putting It All Together - The Training Loop</h3>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        This is where we orchestrate the entire BPE training process. We'll create a main function that runs a loop for a specified number of merges (<code>num_merges</code>). This hyperparameter controls the size of our final vocabulary.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">In each iteration of the loop, the algorithm will perform the following actions:</p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Call <code>get_pair_stats</code> on the current vocabulary to count all adjacent pairs.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Identify the most frequent pair from the statistics. If no pairs are left, the training stops early.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Record this \"best pair\" as a new <strong>merge rule</strong>. The ordered list of these rules is the primary output of our BPE training.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Call <code>merge_vocab</code> to update the vocabulary by merging the best pair into a single token.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Repeat this process until the desired number of merges is complete.</li>\n","    </ol>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        The two final outputs will be the final state of the vocabulary and, more importantly, the ordered list of <strong>merge rules</strong>. This list is what we need to tokenize new, unseen text.\n","    </p>\n","</div>\n"]},{"cell_type":"code","execution_count":4,"id":"4074f7d5","metadata":{"execution":{"iopub.execute_input":"2025-10-26T19:40:55.686806Z","iopub.status.busy":"2025-10-26T19:40:55.686505Z","iopub.status.idle":"2025-10-26T19:40:55.699944Z","shell.execute_reply":"2025-10-26T19:40:55.698926Z"},"papermill":{"duration":0.020577,"end_time":"2025-10-26T19:40:55.701395","exception":false,"start_time":"2025-10-26T19:40:55.680818","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Initial Vocabulary ---\n","{'l o w </w>': 3, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e r </w>': 1}\n","--------------------------------------------------\n","Merge 1/10: Merged ('l', 'o') -> lo\n","  Current Vocab: ['lo w </w>', 'lo w e r </w>', 'n e w e s t </w>', 'w i d e r </w>']\n","\n","Merge 2/10: Merged ('lo', 'w') -> low\n","  Current Vocab: ['low </w>', 'low e r </w>', 'n e w e s t </w>', 'w i d e r </w>']\n","\n","Merge 3/10: Merged ('low', '</w>') -> low</w>\n","  Current Vocab: ['low</w>', 'low e r </w>', 'n e w e s t </w>', 'w i d e r </w>']\n","\n","Merge 4/10: Merged ('e', 'r') -> er\n","  Current Vocab: ['low</w>', 'low er </w>', 'n e w e s t </w>', 'w i d er </w>']\n","\n","Merge 5/10: Merged ('er', '</w>') -> er</w>\n","  Current Vocab: ['low</w>', 'low er</w>', 'n e w e s t </w>', 'w i d er</w>']\n","\n","Merge 6/10: Merged ('low', 'er</w>') -> lower</w>\n","  Current Vocab: ['low</w>', 'lower</w>', 'n e w e s t </w>', 'w i d er</w>']\n","\n","Merge 7/10: Merged ('n', 'e') -> ne\n","  Current Vocab: ['low</w>', 'lower</w>', 'ne w e s t </w>', 'w i d er</w>']\n","\n","Merge 8/10: Merged ('ne', 'w') -> new\n","  Current Vocab: ['low</w>', 'lower</w>', 'new e s t </w>', 'w i d er</w>']\n","\n","Merge 9/10: Merged ('new', 'e') -> newe\n","  Current Vocab: ['low</w>', 'lower</w>', 'newe s t </w>', 'w i d er</w>']\n","\n","Merge 10/10: Merged ('newe', 's') -> newes\n","  Current Vocab: ['low</w>', 'lower</w>', 'newes t </w>', 'w i d er</w>']\n","\n","\n","========================= TRAINING COMPLETE =========================\n","\n","--- Final Vocabulary State ---\n","'low</w>': 3\n","'lower</w>': 1\n","'newes t </w>': 1\n","'w i d er</w>': 1\n","\n","--- Learned Merge Rules (in order) ---\n","1: ('l', 'o')\n","2: ('lo', 'w')\n","3: ('low', '</w>')\n","4: ('e', 'r')\n","5: ('er', '</w>')\n","6: ('low', 'er</w>')\n","7: ('n', 'e')\n","8: ('ne', 'w')\n","9: ('new', 'e')\n","10: ('newe', 's')\n"]}],"source":["import collections\n","import re\n","\n","# We need to include all our helper functions in this cell to make it self-contained\n","# and runnable in the Kaggle notebook.\n","\n","def get_vocab(corpus: str) -> dict:\n","    \"\"\"Creates an initial vocabulary from a raw text corpus.\"\"\"\n","    words = corpus.strip().split()\n","    word_counts = collections.Counter(words)\n","    vocab = {' '.join(list(word)) + ' </w>': count for word, count in word_counts.items()}\n","    return vocab\n","\n","def get_pair_stats(vocab: dict) -> collections.Counter:\n","    \"\"\"Counts the frequency of each adjacent pair of symbols.\"\"\"\n","    pair_counts = collections.Counter()\n","    for word, count in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols) - 1):\n","            pair_counts[(symbols[i], symbols[i+1])] += count\n","    return pair_counts\n","\n","def merge_vocab(pair: tuple, v_in: dict) -> dict:\n","    \"\"\"Merges a specific pair in the vocabulary.\"\"\"\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair))\n","    p = ''.join(pair)\n","    regex = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    for word in v_in:\n","        w_out = regex.sub(p, word)\n","        v_out[w_out] = v_in[word]\n","    return v_out\n","\n","# --- The Main BPE Training Function ---\n","\n","def train_bpe(corpus: str, num_merges: int, verbose: bool = True):\n","    \"\"\"\n","    Trains a BPE model on a corpus.\n","\n","    Args:\n","        corpus (str): The training text.\n","        num_merges (int): The number of merge operations to perform.\n","        verbose (bool): If True, prints the progress of each merge.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - The final vocabulary after all merges.\n","            - A list of the merge rules in the order they were learned.\n","    \"\"\"\n","    # 1. Initialize vocabulary from the corpus\n","    vocab = get_vocab(corpus)\n","    \n","    # This will store our ordered merge rules\n","    merge_rules = []\n","    \n","    if verbose:\n","        print(\"--- Initial Vocabulary ---\")\n","        print(vocab)\n","        print(\"-\" * 50)\n","    \n","    for i in range(num_merges):\n","        # 2. Get pair statistics from the current vocabulary\n","        pair_stats = get_pair_stats(vocab)\n","        \n","        # If there are no more pairs to merge, stop\n","        if not pair_stats:\n","            if verbose: print(\"No more pairs to merge. Stopping early.\")\n","            break\n","        \n","        # 3. Find the most frequent pair\n","        best_pair = max(pair_stats, key=pair_stats.get)\n","        merge_rules.append(best_pair)\n","        \n","        # 4. Merge the best pair in the vocabulary\n","        vocab = merge_vocab(best_pair, vocab)\n","        \n","        if verbose:\n","            print(f\"Merge {i+1}/{num_merges}: Merged {best_pair} -> {''.join(best_pair)}\")\n","            print(f\"  Current Vocab: {list(vocab.keys())}\\n\")\n","            \n","    return vocab, merge_rules\n","\n","# --- Let's run the full training loop on our corpus ---\n","\n","corpus_text = \"low lower newest wider low low\"\n","num_merges_to_perform = 10\n","\n","final_vocab, learned_rules = train_bpe(corpus_text, num_merges_to_perform)\n","\n","print(\"\\n\" + \"=\"*25 + \" TRAINING COMPLETE \" + \"=\"*25)\n","print(\"\\n--- Final Vocabulary State ---\")\n","for word, count in final_vocab.items():\n","    print(f\"'{word}': {count}\")\n","\n","print(\"\\n--- Learned Merge Rules (in order) ---\")\n","for i, rule in enumerate(learned_rules):\n","    print(f\"{i+1}: {rule}\")\n"]},{"cell_type":"markdown","id":"c47dde5f","metadata":{"papermill":{"duration":0.004348,"end_time":"2025-10-26T19:40:55.710408","exception":false,"start_time":"2025-10-26T19:40:55.70606","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Part 5: Applying the BPE Model to Tokenize New Words</h3>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        Now that we have our ordered list of <code>learned_rules</code>, we can build a tokenizer function. This function will take a new word (or a list of words) and segment it into the subword tokens our model has learned. This is the \"inference\" step.\n","    </p>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        The process for tokenizing a single new word is as follows:\n","    </p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">First, split the word into its basic characters and add the end-of-word marker <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>. For example, \"lowest\" becomes <span class=\"bpe-kbd-dark\">l o w e s t &lt;/w&gt;</span>.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Iterate through our <strong>ordered</strong> <code>learned_rules</code> one by one.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">For each rule (e.g., merge <span class=\"bpe-kbd-dark\">('l', 'o')</span> into <span class=\"bpe-kbd-dark\">lo</span>), find the most frequent adjacent pair within the current tokenization of the word.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">If the most frequent pair in the word is the same as the current merge rule we are applying, merge it. Then, restart the search for the next best pair <em>from the beginning of the merge rules list</em>. This ensures that higher-priority merges are always performed first.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">If the best pair in the word is not our current rule, we simply move to the next rule in the list.\n","        </li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Continue this process until no more pairs in the word can be merged according to our rules.</li>\n","    </ol>\n","    <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This procedure allows BPE to handle out-of-vocabulary (OOV) words gracefully. For instance, the word \"lowest\" was not in our training data, but because it contains subwords like \"low\" and \"est\", our tokenizer can break it down into meaningful, learned pieces like <span class=\"bpe-kbd-dark\">low</span>, <span class=\"bpe-kbd-dark\">es</span>, and <span class=\"bpe-kbd-dark\">t&lt;/w&gt;</span> (depending on the learned rules).\n","    </p>\n","</div>\n"]},{"cell_type":"code","execution_count":5,"id":"06239033","metadata":{"execution":{"iopub.execute_input":"2025-10-26T19:40:55.721099Z","iopub.status.busy":"2025-10-26T19:40:55.72035Z","iopub.status.idle":"2025-10-26T19:40:55.730852Z","shell.execute_reply":"2025-10-26T19:40:55.729848Z"},"papermill":{"duration":0.017453,"end_time":"2025-10-26T19:40:55.732281","exception":false,"start_time":"2025-10-26T19:40:55.714828","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenization of 'lower': ['lower</w>']\n","Tokenization of 'lowest': ['low', 'e', 's', 't', '</w>']\n","Tokenization of 'newer': ['new', 'er</w>']\n","Tokenization of 'know': ['k', 'n', 'o', 'w', '</w>']\n"]}],"source":["import collections\n","\n","# We need a helper to find the best pair within a single word's tokenization\n","def get_word_pair_stats(tokens: list) -> collections.Counter:\n","    \"\"\"Counts adjacent pairs in a single tokenized word.\"\"\"\n","    pair_counts = collections.Counter()\n","    for i in range(len(tokens) - 1):\n","        pair_counts[(tokens[i], tokens[i+1])] += 1\n","    return pair_counts\n","\n","# --- The BPE Tokenizer Function ---\n","\n","def tokenize_word(word: str, learned_rules: list) -> list:\n","    \"\"\"\n","    Tokenizes a single word using a pre-learned list of BPE merge rules.\n","    \n","    Args:\n","        word (str): The word to tokenize.\n","        learned_rules (list): The ordered list of merge rules.\n","        \n","    Returns:\n","        list: A list of BPE tokens for the word.\n","    \"\"\"\n","    # 1. Pre-process the word: split into chars and add end-of-word marker\n","    tokens = list(word) + ['</w>']\n","    \n","    while True:\n","        # Find the most frequent pair in the current tokenization\n","        pair_stats = get_word_pair_stats(tokens)\n","        \n","        # If there are no pairs, we're done with this word\n","        if not pair_stats:\n","            break\n","            \n","        # Find the pair that should be merged next by checking against the ordered rules\n","        # We look for the first rule in `learned_rules` that exists in our current `pair_stats`\n","        best_pair_to_merge = None\n","        for rule in learned_rules:\n","            if rule in pair_stats:\n","                best_pair_to_merge = rule\n","                break\n","        \n","        # If no pair from our rules is found in the current word tokens, we're done.\n","        if best_pair_to_merge is None:\n","            break\n","\n","        # Merge the best pair\n","        new_tokens = []\n","        i = 0\n","        while i < len(tokens):\n","            # Find the first occurrence of the pair to merge\n","            try:\n","                j = tokens.index(best_pair_to_merge[0], i)\n","                # Check if it's a valid pair (the next token matches)\n","                if j + 1 < len(tokens) and tokens[j+1] == best_pair_to_merge[1]:\n","                    # Append tokens before the pair\n","                    new_tokens.extend(tokens[i:j])\n","                    # Append the merged token\n","                    new_tokens.append(''.join(best_pair_to_merge))\n","                    # Move the index past the merged pair\n","                    i = j + 2\n","                else:\n","                    # Not a valid pair, just append the token and continue\n","                    new_tokens.append(tokens[i])\n","                    i += 1\n","            except ValueError:\n","                # The first element of the pair was not found, so append the rest of the list\n","                new_tokens.extend(tokens[i:])\n","                break\n","        \n","        tokens = new_tokens\n","\n","    return tokens\n","\n","# --- Let's test our tokenizer! ---\n","\n","# We use the 'learned_rules' from the previous step.\n","# If running this cell alone, uncomment the line below:\n","# learned_rules = [('l', 'o'), ('lo', 'w'), ('low', '</w>'), ('e', 'r'), ('er', '</w>'), ('low', 'er</w>'), ('n', 'e'), ('ne', 'w'), ('new', 'e'), ('newe', 's')]\n","\n","# Let's tokenize a word that was in our training set: \"lower\"\n","tokenized_lower = tokenize_word(\"lower\", learned_rules)\n","print(f\"Tokenization of 'lower': {tokenized_lower}\")\n","\n","# Now, let's tokenize a new, unseen word: \"lowest\"\n","tokenized_lowest = tokenize_word(\"lowest\", learned_rules)\n","print(f\"Tokenization of 'lowest': {tokenized_lowest}\")\n","\n","# Another unseen word: \"newer\"\n","tokenized_newer = tokenize_word(\"newer\", learned_rules)\n","print(f\"Tokenization of 'newer': {tokenized_newer}\")\n","\n","# A word that can't be merged much: \"know\"\n","tokenized_know = tokenize_word(\"know\", learned_rules)\n","print(f\"Tokenization of 'know': {tokenized_know}\")\n"]},{"cell_type":"markdown","id":"c769bcbe","metadata":{"papermill":{"duration":0.004378,"end_time":"2025-10-26T19:40:55.741552","exception":false,"start_time":"2025-10-26T19:40:55.737174","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Part 6: Understanding the Tokenizer's Output</h3>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n","<p style=\"color: #a0aec0;\">\n","        The output from our tokenizer function demonstrates the core principles of BPE. The final tokenization of each word is a direct result of applying our 10 learned merge rules in their specific order. Let's analyze each case to see how the model arrived at its decision.\n","    </p>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #63b3ed; padding: 15px;\">\n","        <h4 style=\"color: #90cdf4; margin: 0 0 10px 0;\">1. Word: \"lower\" &rarr; Result: <code>['lower&lt;/w&gt;']</code></h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This is a perfect example of a word our model is \"expert\" at. It was present in our training corpus, so the model learned all the necessary merges to represent it as a single token:</p>\n","        <ol style=\"color: #a0aec0; padding-left: 25px; font-size: 0.95em;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\"><code>l o w e r &lt;/w&gt;</code> &rarr; applies rule #1 <code>('l','o')</code> &rarr; <code>lo w e r &lt;/w&gt;</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\">&rarr; applies rule #2 <code>('lo','w')</code> &rarr; <code>low e r &lt;/w&gt;</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\">&rarr; applies rule #4 <code>('e','r')</code> &rarr; <code>low er &lt;/w&gt;</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\">&rarr; applies rule #5 <code>('er','&lt;/w&gt;')</code> &rarr; <code>low er&lt;/w&gt;</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\">&rarr; applies rule #6 <code>('low','er&lt;/w&gt;')</code> &rarr; <code>lower&lt;/w&gt;</code></li>\n","        </ol>\n","        <p style=\"margin: 0; color: #a0aec0;\">Since all sub-parts could be progressively merged into one, the final output is a single token.</p>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #f6ad55; padding: 15px;\">\n","        <h4 style=\"color: #fbd38d; margin: 0 0 10px 0;\">2. Word: \"lowest\" &rarr; Result: <code>['low', 'e', 's', 't', '&lt;/w&gt;']</code></h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This is an out-of-vocabulary (OOV) word. The tokenizer does its best with the rules it knows:</p>\n","        <ol style=\"color: #a0aec0; padding-left: 25px; font-size: 0.95em;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\"><code>l o w e s t &lt;/w&gt;</code> &rarr; applies rules #1 and #2 &rarr; <code>low e s t &lt;/w&gt;</code></li>\n","        </ol>\n","        <p style=\"margin: 0; color: #a0aec0;\">At this point, the tokens are <span class=\"bpe-kbd-dark\">low</span>, <span class=\"bpe-kbd-dark\">e</span>, <span class=\"bpe-kbd-dark\">s</span>, <span class=\"bpe-kbd-dark\">t</span>, <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>. The tokenizer looks for pairs like <span class=\"bpe-kbd-dark\">('low', 'e')</span>, <span class=\"bpe-kbd-dark\">('e', 's')</span>, <span class=\"bpe-kbd-dark\">('s', 't')</span>, etc. <strong>None of these pairs exist in our 10 learned rules.</strong> For example, we learned to merge <span class=\"bpe-kbd-dark\">('new', 'e')</span> but not <span class=\"bpe-kbd-dark\">('low', 'e')</span>. Therefore, the merging process stops here.\n","        </p>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #81e6d9; padding: 15px;\">\n","        <h4 style=\"color: #a7f3d0; margin: 0 0 10px 0;\">3. Word: \"newer\" &rarr; Result: <code>['new', 'er&lt;/w&gt;']</code></h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This is another great example of handling an OOV word by composing known subwords. \"newer\" was not in our training data, but \"newest\" and \"lower/wider\" were.</p>\n","        <ol style=\"color: #a0aec0; padding-left: 25px; font-size: 0.95em;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\"><code>n e w e r &lt;/w&gt;</code> &rarr; applies rules #7 and #8 &rarr; <code>new e r &lt;/w&gt;</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\">&rarr; applies rule #4 &rarr; <code>new er &lt;/w&gt;</code></li>\n","            <li style=\"padding-left: 10px; margin-bottom: 5px;\">&rarr; applies rule #5 &rarr; <code>new er&lt;/w&gt;</code></li>\n","        </ol>\n","        <p style=\"margin: 0; color: #a0aec0;\">The process stops here because the pair <span class=\"bpe-kbd-dark\">('new', 'er&lt;/w&gt;')</span> was never learned. Our model only learned to merge <span class=\"bpe-kbd-dark\">('low', 'er&lt;/w&gt;')</span>. This shows how BPE captures both stem words (<span class=\"bpe-kbd-dark\">new</span>) and common suffixes (<span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>).</p>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #f56565; padding: 15px;\">\n","        <h4 style=\"color: #fc8181; margin: 0 0 10px 0;\">4. Word: \"know\" &rarr; Result: <code>['k', 'n', 'o', 'w', '&lt;/w&gt;']</code></h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This word demonstrates the ultimate fallback: if a word contains no known sub-sequences, it's simply broken down into its individual characters.\n","        The initial tokens are <span class=\"bpe-kbd-dark\">k</span>, <span class=\"bpe-kbd-dark\">n</span>, <span class=\"bpe-kbd-dark\">o</span>, <span class=\"bpe-kbd-dark\">w</span>, <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>. The tokenizer looks at the pairs <span class=\"bpe-kbd-dark\">('k', 'n')</span>, <span class=\"bpe-kbd-dark\">('n', 'o')</span>, <span class=\"bpe-kbd-dark\">('o', 'w')</span>, etc. Not a single one of these pairs exists in our `learned_rules`. Therefore, no merges can be performed, and the word remains fully split.\n","        </p>\n","    </div>\n","</div>\n"]},{"cell_type":"markdown","id":"8c42c92b","metadata":{"papermill":{"duration":0.004316,"end_time":"2025-10-26T19:40:55.750418","exception":false,"start_time":"2025-10-26T19:40:55.746102","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Part 7: Where to Go From Here?</h3>\n","\n","<p style=\"color: #a0aec0;\">\n","        We've successfully built a BPE tokenizer. Now, the real fun begins! We can use this tokenizer as a foundational piece for more advanced NLP models or explore ways to improve the tokenization process itself. Below are three potential paths to continue this project, ranging from direct improvements to building a full language model.\n","    </p>\n","\n","<div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #63b3ed; padding: 20px; border-radius: 5px;\">\n","        <h4 style=\"color: #90cdf4; margin: 0 0 10px 0;\">Path 1: Building a Text Encoder/Decoder</h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">\n","            Our current tokenizer works on single words. A complete tokenizer needs to handle entire sentences or documents. This involves creating a full \"Encoder\" class that manages the vocabulary and converts text to integer sequences, and a \"Decoder\" to convert them back.\n","        </p>\n","        <ul style=\"color: #a0aec0; padding-left: 25px; font-size: 0.95em;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Create a Vocabulary Lookup:</strong> After training, create a final vocabulary list (<code>vocab</code>) that maps each unique token (from the initial characters and all merged tokens) to an integer index. Example: <code>{'l': 0, 'o': 1, ..., 'low': 27, 'er&lt;/w&gt;': 28, ...}</code>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Build the <code>encode()</code> method:</strong> This method will take a string of text, split it into words, tokenize each word using our <code>tokenize_word</code> function, and then map each resulting subword token to its integer ID from the vocabulary lookup.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Build the <code>decode()</code> method:</strong> This method will take a list of integers, map them back to their string tokens, and then join them to reconstruct the original text. You'll need to handle the <code>&lt;/w&gt;</code> tokens by replacing them with a space.</li>\n","        </ul>\n","        <p style=\"margin: 15px 0 0 0; color: #a0aec0;\">\n","            <strong>Outcome:</strong> A self-contained Python class that can encode any text into numbers and decode them back, just like tokenizers from libraries like Hugging Face's `tokenizers`.\n","        </p>\n","    </div>\n","\n","<div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #f6ad55; padding: 20px; border-radius: 5px;\">\n","        <h4 style=\"color: #fbd38d; margin: 0 0 10px 0;\">Path 2: Build a Simple Language Model (N-gram)</h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">\n","            With your text encoded into token IDs, you can build a classical statistical language model. An N-gram model learns to predict the next token based on the previous <em>N-1</em> tokens. A bigram (N=2) model is a great place to start.\n","        </p>\n","        <ul style=\"color: #a0aec0; padding-left: 25px; font-size: 0.95em;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Encode your corpus:</strong> Use the encoder from Path 1 to convert your entire training corpus into a long sequence of token IDs.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Count Bigram Frequencies:</strong> Iterate through the sequence and count the occurrences of every adjacent pair of token IDs. For example, how many times does token `27` (`low`) appear before token `15` (`e`)? Store this in a nested dictionary or a matrix.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Calculate Probabilities:</strong> Convert these counts into conditional probabilities: $P(\\text{token}_i | \\text{token}_{i-1})$.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Generate Text:</strong> Create a function that starts with a seed token, predicts the most likely next token, appends it, and then uses that new token to predict the next one, generating new text one token at a time.</li>\n","        </ul>\n","        <p style=\"margin: 15px 0 0 0; color: #a0aec0;\">\n","            <strong>Outcome:</strong> A generative model that can produce novel text that mimics the style of your training corpus. It's the \"ancestor\" of modern models like GPT.\n","        </p>\n","    </div>\n","\n","<div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #81e6d9; padding: 20px; border-radius: 5px;\">\n","        <h4 style=\"color: #a7f3d0; margin: 0 0 10px 0;\">Path 3: Text Classification with Embeddings</h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">\n","            This is a more modern approach. You can use your tokenizer to prepare data for a neural network that performs a task like sentiment analysis.\n","        </p>\n","        <ul style=\"color: #a0aec0; padding-left: 25px; font-size: 0.95em;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Find a Labeled Dataset:</strong> Use a dataset like the IMDB movie reviews, which has text and a positive/negative label.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Train the BPE Tokenizer:</strong> Train your BPE tokenizer on the text from this dataset to create a domain-specific vocabulary.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Build a Neural Network Model:</strong> Using a library like TensorFlow or PyTorch, build a simple model. The first layer will be an <strong>Embedding layer</strong>. This layer maps each of your integer token IDs to a dense vector, turning <code>[27, 15, 83]</code> into a matrix of float vectors.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Train the Classifier:</strong> Feed the embedded token sequences into subsequent layers (like LSTM, GRU, or even just a simple pooling layer) and train the network to predict the sentiment label. The embedding vectors will be learned automatically during this process.</li>\n","        </ul>\n","        <p style=\"margin: 15px 0 0 0; color: #a0aec0;\">\n","            <strong>Outcome:</strong> A complete text classification pipeline, where you've built the tokenizer from scratch. You'll gain a deep understanding of how text is prepared for modern deep learning models.\n","        </p>\n","    </div>\n","</div>\n"]},{"cell_type":"markdown","id":"01100ee9","metadata":{"papermill":{"duration":0.004285,"end_time":"2025-10-26T19:40:55.759167","exception":false,"start_time":"2025-10-26T19:40:55.754882","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Part 8: Encapsulating BPE into a Reusable Tokenizer Class</h3>\n","\n","<p style=\"color: #a0aec0;\">\n","        So far, we have a set of powerful but disconnected functions: one to train BPE rules (`train_bpe`) and another to tokenize a single word (`tokenize_word`). This is like having the engine of a car but no chassis, steering wheel, or pedals. To make our tokenizer truly useful, we need to build a complete \"vehicle\" around it—a self-contained class that handles the entire process from raw text to numerical data and back again.\n","    </p>\n","\n","<p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This class, which we'll call <code>BPE_Tokenizer</code>, will manage three critical components:\n","    </p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Merge Rules:</strong> The ordered list of merge rules we learned during training.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Vocabulary Lookup:</strong> A mapping from each unique subword token (like 'low', 'er</w>') to a unique integer ID. This is what allows us to convert text into a format that machine learning models can understand.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Encoder/Decoder Logic:</strong> The main <code>encode()</code> and <code>decode()</code> methods that serve as the public interface for our tokenizer.</li>\n","    </ol>\n","\n","<h4 style=\"color: #a0aec0; margin-top: 25px;\">Pros & Cons: Moving from Raw Functions to a Class Structure</h4>\n","\n","<div style=\"display: flex; gap: 20px; margin-top: 15px;\">\n","        <div style=\"flex: 1; background-color: #1a202c; border: 1px solid #38a169; border-radius: 8px; padding: 15px;\">\n","            <h5 style=\"color: #68d391; margin: 0 0 10px 0; font-size: 1.1em;\">Pros (Why a Class is Better)</h5>\n","            <ul style=\"padding-left: 20px; margin: 0; color: #a0aec0; font-size: 0.95em;\">\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>State Management:</strong> The class holds the `learned_rules` and `vocab` as internal state (attributes). We no longer need to pass them around as function arguments, which is cleaner and less error-prone.\n","                </li>\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>Abstraction:</strong> A user of the class only needs to know about <code>.train()</code>, <code>.encode()</code>, and <code>.decode()</code>. The complex internal logic of merging pairs and handling word boundaries is hidden away. This is a core principle of good software design.\n","                </li>\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>Reusability & Portability:</strong> The trained tokenizer object can be easily saved (e.g., using `pickle`) and loaded in another project without needing to retrain or copy-paste all the helper functions. It becomes a single, portable artifact.\n","                </li>\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>Handles Full Sentences:</strong> Our previous `tokenize_word` function only worked on a single word. The `encode` method in the class will be designed to process entire strings of text, handling spaces and punctuation correctly.\n","                </li>\n","            </ul>\n","        </div>\n","\n"," <div style=\"flex: 1; background-color: #1a202c; border: 1px solid #c53030; border-radius: 8px; padding: 15px;\">\n","            <h5 style=\"color: #f56565; margin: 0 0 10px 0; font-size: 1.1em;\">Cons (Potential Downsides)</h5>\n","            <ul style=\"padding-left: 20px; margin: 0; color: #a0aec0; font-size: 0.95em;\">\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>Increased Complexity (Initially):</strong> Writing a class requires more boilerplate code than simple functions. We need to think about the `__init__` method and how the object's attributes relate to each other.\n","                </li>\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>Less Transparency (for Debugging):</strong> While abstraction is a pro for users, it can be a con for developers trying to debug. The logic is now \"hidden\" inside methods, which can make tracing data flow slightly more challenging than with standalone functions.\n","                </li>\n","                <li style=\"margin-bottom: 10px;\">\n","                    <strong>Stateful is not always better:</strong> For very simple, one-off scripts, a functional approach can be more straightforward. An object holding state can sometimes lead to unexpected behavior if its state is modified incorrectly. (However, for a tokenizer, state is essential).\n","                </li>\n","            </ul>\n","        </div>\n","    </div>\n","    <p style=\"margin-top:20px; color: #cbd5e1; text-align: center;\">\n","        Overall, for any serious application, the benefits of encapsulation overwhelmingly outweigh the drawbacks. We are now moving from a \"script\" to a \"tool\".\n","    </p>\n","\n","</div>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":5.739434,"end_time":"2025-10-26T19:40:56.18201","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-26T19:40:50.442576","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}