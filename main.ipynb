{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/the-art-of-tokenization?scriptVersionId=270367606\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"9e6466f5","metadata":{"papermill":{"duration":0.002453,"end_time":"2025-10-23T20:00:22.6089","exception":false,"start_time":"2025-10-23T20:00:22.606447","status":"completed"},"tags":[]},"source":["# The Art of Tokenization, Part 1: Byte Pair Encoding"]},{"cell_type":"markdown","id":"4dbc2a84","metadata":{"papermill":{"duration":0.001252,"end_time":"2025-10-23T20:00:22.612192","exception":false,"start_time":"2025-10-23T20:00:22.61094","status":"completed"},"tags":[]},"source":["Welcome to this series on the art and science of tokenization! In Natural Language Processing (NLP), before a powerful model like GPT or BERT can understand human language, we must first break that text down into pieces it can recognize. These pieces are called tokens. The process of creating them is tokenization, and it’s one of the most fundamental steps in any NLP pipeline.\n","\n","This series will explore the different strategies for tokenization. We’ll start with one of the most influential methods in the modern NLP landscape: Byte Pair Encoding (BPE)."]},{"cell_type":"markdown","id":"c7eebe33","metadata":{"papermill":{"duration":0.00119,"end_time":"2025-10-23T20:00:22.614908","exception":false,"start_time":"2025-10-23T20:00:22.613718","status":"completed"},"tags":[]},"source":["# What is BPE? The “Happy Medium” of Tokenization\n","At its core, Byte Pair Encoding (BPE) is a subword tokenization algorithm. Instead of forcing us to choose between whole words or individual characters, it finds a “happy medium.”\n"]},{"cell_type":"markdown","id":"5bf38336","metadata":{"papermill":{"duration":0.001132,"end_time":"2025-10-23T20:00:22.617493","exception":false,"start_time":"2025-10-23T20:00:22.616361","status":"completed"},"tags":[]},"source":["Imagine trying to create a dictionary for a language model:\n","* Option A: Word Dictionary. You include every single word (\"cat\", \"run\", \"photosynthesis\", \"antidisestablishmentarianism\").\n","\n","* Problem: The dictionary becomes enormous. What about new words (“de-platforming”), slang (“yeet”), or simple typos (“helllo”)? They are all “Out-of-Vocabulary” (OOV) and become a meaningless <UNK> (unknown) token.\n","\n","* Option B: Character Dictionary. You only include characters ('a', 'b', 'c', '!').\n","\n","* Problem: No more OOV issues, but the text sequences become incredibly long. The sentence “Hello world” is now 11 tokens ('H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'). This makes it computationally expensive and harder for the model to find meaning."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":6.157017,"end_time":"2025-10-23T20:00:23.041068","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-23T20:00:16.884051","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}