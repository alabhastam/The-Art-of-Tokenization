{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/the-art-of-tokenization?scriptVersionId=270739935\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"eb0baed6","metadata":{"papermill":{"duration":0.003052,"end_time":"2025-10-25T10:09:27.702339","exception":false,"start_time":"2025-10-25T10:09:27.699287","status":"completed"},"tags":[]},"source":["# The Art of Tokenization, Part 1: Byte Pair Encoding"]},{"cell_type":"markdown","id":"f020e621","metadata":{"papermill":{"duration":0.002055,"end_time":"2025-10-25T10:09:27.707137","exception":false,"start_time":"2025-10-25T10:09:27.705082","status":"completed"},"tags":[]},"source":["Welcome to this series on the art and science of tokenization! In Natural Language Processing (NLP), before a powerful model like GPT or BERT can understand human language, we must first break that text down into pieces it can recognize. These pieces are called tokens. The process of creating them is tokenization, and it’s one of the most fundamental steps in any NLP pipeline.\n","\n","This series will explore the different strategies for tokenization. We’ll start with one of the most influential methods in the modern NLP landscape: Byte Pair Encoding (BPE)."]},{"cell_type":"markdown","id":"5cdf6e05","metadata":{"papermill":{"duration":0.001974,"end_time":"2025-10-25T10:09:27.711305","exception":false,"start_time":"2025-10-25T10:09:27.709331","status":"completed"},"tags":[]},"source":["# What is BPE? The “Happy Medium” of Tokenization\n","At its core, Byte Pair Encoding (BPE) is a subword tokenization algorithm. Instead of forcing us to choose between whole words or individual characters, it finds a “happy medium.”\n"]},{"cell_type":"markdown","id":"837e6120","metadata":{"papermill":{"duration":0.001985,"end_time":"2025-10-25T10:09:27.715485","exception":false,"start_time":"2025-10-25T10:09:27.7135","status":"completed"},"tags":[]},"source":["Imagine trying to create a dictionary for a language model:\n","* Option A: Word Dictionary. You include every single word (\"cat\", \"run\", \"photosynthesis\", \"antidisestablishmentarianism\").\n","\n","* Problem: The dictionary becomes enormous. What about new words (“de-platforming”), slang (“yeet”), or simple typos (“helllo”)? They are all “Out-of-Vocabulary” (OOV) and become a meaningless <UNK> (unknown) token.\n","\n","* Option B: Character Dictionary. You only include characters ('a', 'b', 'c', '!').\n","\n","* Problem: No more OOV issues, but the text sequences become incredibly long. The sentence “Hello world” is now 11 tokens ('H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'). This makes it computationally expensive and harder for the model to find meaning."]},{"cell_type":"markdown","id":"21e2fceb","metadata":{"papermill":{"duration":0.001944,"end_time":"2025-10-25T10:09:27.719642","exception":false,"start_time":"2025-10-25T10:09:27.717698","status":"completed"},"tags":[]},"source":["This way, a common word like \"where\" can be a single token, while a rare, complex word like \"retrofitting\" can be broken down into meaningful pieces like [\"retro\", \"fit\", \"ting\"]. Crucially, no word is ever “unknown.” Any new word can be built from these subword pieces."]},{"cell_type":"markdown","id":"47ea4a16","metadata":{"papermill":{"duration":0.00194,"end_time":"2025-10-25T10:09:27.72368","exception":false,"start_time":"2025-10-25T10:09:27.72174","status":"completed"},"tags":[]},"source":["# How the BPE Algorithm Works (Conceptually)\n","BPE was originally a data compression algorithm. Its adaptation for NLP is based on a simple, greedy, and iterative idea:\n","> Core Logic: Continuously find the most common pair of adjacent tokens in your text and merge them into a single, new token.\n"]},{"cell_type":"markdown","id":"918fdd52","metadata":{"papermill":{"duration":0.001929,"end_time":"2025-10-25T10:09:27.727712","exception":false,"start_time":"2025-10-25T10:09:27.725783","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">How the BPE Algorithm Works (Conceptually)</h3>\n","    <p style=\"color: #a0aec0;\">BPE is based on a simple, greedy, and iterative idea: continuously find the most common pair of adjacent tokens and merge them. Let's trace the process.</p>\n","\n"," <style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n","<div style=\"margin-top: 25px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 0: Initialize</h4>\n","        <p style=\"margin: 0;\">\n","            First, we break every word down into its basic characters. We also add a special symbol, like <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>, to mark the end of a word. This is important to distinguish between <span class=\"bpe-kbd-dark\">er</span> inside a word (like in “newest”) and <span class=\"bpe-kbd-dark\">er</span> at the end of a word (like in “lower”).\n","        </p>\n","        <p style=\"margin-top: 10px;\">\n","            Our initial “tokens” are just characters: <span class=\"bpe-kbd-dark\">l</span> <span class=\"bpe-kbd-dark\">o</span> <span class=\"bpe-kbd-dark\">w</span> <span class=\"bpe-kbd-dark\">e</span> <span class=\"bpe-kbd-dark\">r</span> <span class=\"bpe-kbd-dark\">n</span> <span class=\"bpe-kbd-dark\">s</span> <span class=\"bpe-kbd-dark\">t</span> <span class=\"bpe-kbd-dark\">i</span> <span class=\"bpe-kbd-dark\">d</span> <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>.\n","        </p>\n","    </div>\n","\n","<div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 1: First Merge</h4>\n","        <p>The algorithm scans the entire corpus and counts the frequency of every adjacent pair of tokens.</p>\n","        <p>Let’s say it finds that the pair <span class=\"bpe-kbd-dark\">e</span> followed by <span class=\"bpe-kbd-dark\">r</span> (i.e., <span class=\"bpe-kbd-dark\">e r</span>) is the most common combination (appearing in <em>lower</em> and <em>wider</em>).</p>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Action:</strong> It merges <span class=\"bpe-kbd-dark\">e</span> and <span class=\"bpe-kbd-dark\">r</span> to create a new token, <span class=\"bpe-kbd-dark\">er</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Vocabulary:</strong> Our vocabulary now includes <span class=\"bpe-kbd-dark\">er</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Corpus Update:</strong> All instances of <span class=\"bpe-kbd-dark\">e r</span> are replaced. So, <span class=\"bpe-kbd-dark\">l o w e r &lt;/w&gt;</span> becomes <span class=\"bpe-kbd-dark\">l o w er &lt;/w&gt;</span>.</li>\n","        </ul>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 2: Second Merge</h4>\n","        <p>The process repeats. The algorithm scans the <em>updated</em> corpus. Perhaps it now finds that <span class=\"bpe-kbd-dark\">er</span> followed by <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span> (i.e., <span class=\"bpe-kbd-dark\">er &lt;/w&gt;</span>) is the most frequent pair.</p>\n","        <ul style=\"list-style-type: '➡️'; padding-left: 20px; margin-top: 10px;\">\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Action:</strong> It merges <span class=\"bpe-kbd-dark\">er</span> and <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span> to create a new token, <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>New Vocabulary:</strong> Our vocabulary now includes <span class=\"bpe-kbd-dark\">er</span> and <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span>.</li>\n","            <li style=\"padding-left: 10px; margin-bottom: 8px;\"><strong>Corpus Update:</strong> <span class=\"bpe-kbd-dark\">l o w er &lt;/w&gt;</span> becomes <span class=\"bpe-kbd-dark\">l o w er&lt;/w&gt;</span>.</li>\n","        </ul>\n","    </div>\n","    \n"," <div style=\"margin-top: 25px; border-top: 1px dashed #4a5568; padding-top: 20px;\">\n","        <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">Step 3: Keep Going…</h4>\n","        <p>Next, maybe <span class=\"bpe-kbd-dark\">l</span> followed by <span class=\"bpe-kbd-dark\">o</span> (<span class=\"bpe-kbd-dark\">l o</span>) is the most common. They get merged into <span class=\"bpe-kbd-dark\">lo</span>. Then <span class=\"bpe-kbd-dark\">lo</span> and <span class=\"bpe-kbd-dark\">w</span> get merged into <span class=\"bpe-kbd-dark\">low</span>.</p>\n","    </div>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">The End Result</h4>\n","        <p style=\"margin: 0; color: #a0aec0;\">This process is repeated for a predetermined number of merges (e.g., <strong>30,000 times</strong>). The final vocabulary consists of the initial characters plus all the new tokens created during the merges. This gives us a ranked list of merge rules that we can use to tokenize any new text.</p>\n","    </div>\n","</div>\n"]},{"cell_type":"markdown","id":"3037ce6e","metadata":{"papermill":{"duration":0.002109,"end_time":"2025-10-25T10:09:27.731932","exception":false,"start_time":"2025-10-25T10:09:27.729823","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","  <h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">\n","    Pros and Cons of BPE\n","  </h3>\n","  <p style=\"color: #a0aec0;\">\n","    BPE is widely used for a reason, but it’s not without its trade‑offs.\n","  </p>\n","\n","  <style>\n","    .bpe-kbd-dark {\n","      background-color: #1a202c;\n","      border: 1px solid #4a5568;\n","      border-bottom: 2px solid #718096;\n","      border-radius: 4px;\n","      padding: 3px 6px;\n","      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","      font-size: 0.9em;\n","      color: #cbd5e1;\n","      white-space: nowrap;\n","    }\n","  </style>\n","\n","  <!-- ✅ PROS SECTION -->\n","  <div style=\"margin-top: 25px;\">\n","    <h4 style=\"color: #68d391; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      Advantages (Pros) 👍\n","    </h4>\n","    <ul style=\"list-style-type: '✔️'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Eliminates Out‑of‑Vocabulary (OOV) Words:</strong> Any new or rare word can be decomposed into a sequence of known subword tokens. The model never has to deal with a completely \n","        <span class=\"bpe-kbd-dark\">&lt;UNK&gt;</span> token.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Controllable Vocabulary Size:</strong> The final vocabulary size is a hyperparameter — the number of merges. You can balance model size and expressiveness.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Efficiency:</strong> Common words stay as single tokens, making sequences shorter and inference faster. Rare words are decomposed smartly, making better use of vocab space.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Captures Morphology:</strong> Learns meaningful parts like prefixes (<span class=\"bpe-kbd-dark\">un-</span>), suffixes (<span class=\"bpe-kbd-dark\">-ing</span>), and stems, helping generalization. If it learns \n","        <span class=\"bpe-kbd-dark\">un</span>, it can recognize \n","        <em>unhappy</em>, <em>unclear</em>, <em>unbelievable</em>.\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- ❌ CONS SECTION -->\n","  <div style=\"margin-top: 35px; border-top: 1px dashed #4a5568; padding-top: 25px;\">\n","    <h4 style=\"color: #f56565; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      Disadvantages (Cons) 👎\n","    </h4>\n","    <ul style=\"list-style-type: '⚠️'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Greedy Approach:</strong> BPE makes local (greedy) merges — not necessarily globally optimal. A different merge order might yield a better vocabulary.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>Sensitive to Training Data:</strong> Merge rules depend entirely on the training corpus. A BPE trained on Wikipedia would tokenize Twitter slang or biomedical text poorly.\n","      </li>\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>The “Byte” Misnomer:</strong> Most modern BPE operates on characters, not raw bytes. True byte‑level BPE (used in GPT‑2) works directly on UTF‑8 bytes and is more robust — handling any text, emoji, or noise.\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- 🌍 CONTEXT SECTION -->\n","  <div style=\"margin-top: 35px; border-top: 1px dashed #4a5568; padding-top: 25px;\">\n","    <h4 style=\"color: #b794f4; border-bottom: 1px solid #4a5568; padding-bottom: 5px;\">\n","      BPE’s Place in the Tokenization World\n","    </h4>\n","    <p style=\"color: #a0aec0;\">\n","      BPE is a cornerstone method for subword tokenization, but it’s not alone. Its relatives include:\n","    </p>\n","    <ul style=\"list-style-type: '🔹'; padding-left: 25px; margin-top: 15px;\">\n","      <li style=\"margin-bottom: 10px;\">\n","        <strong>WordPiece</strong> (<span class=\"bpe-kbd-dark\">BERT</span>, <span class=\"bpe-kbd-dark\">DistilBERT</span>): Similar to BPE but merges based on the merge that <em>maximizes the likelihood</em> of the data instead of raw frequency.\n","      </li>\n","      <li>\n","        <strong>Unigram Language Model</strong> (<span class=\"bpe-kbd-dark\">T5</span>, <span class=\"bpe-kbd-dark\">ALBERT</span>, <span class=\"bpe-kbd-dark\">XLNet</span>): A probabilistic approach that prunes subwords contributing least to total corpus probability — can produce multiple valid tokenizations per word (useful as regularization).\n","      </li>\n","    </ul>\n","  </div>\n","\n","  <!-- ⚡ FINAL NOTE -->\n","  <div style=\"margin-top: 30px; background-color: #1a202c; border-left: 5px solid #3182ce; padding: 15px; border-radius: 6px;\">\n","    <p style=\"margin: 0; color: #a0aec0;\">\n","      Now that we understand the theory, let’s roll up our sleeves and build a \n","      <strong style=\"color:#63b3ed;\">BPE tokenizer</strong> from scratch 🔧\n","    </p>\n","  </div>\n","</div>\n"]},{"cell_type":"markdown","id":"036f00b9","metadata":{"papermill":{"duration":0.00194,"end_time":"2025-10-25T10:09:27.736041","exception":false,"start_time":"2025-10-25T10:09:27.734101","status":"completed"},"tags":[]},"source":["<div style=\"background-color: #2d3748; color: #e2e8f0; border: 1px solid #4a5568; border-radius: 10px; padding: 25px; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7;\">\n","\n","<h3 style=\"color: #90cdf4; border-bottom: 2px solid #4a5568; padding-bottom: 10px; margin-top: 0;\">Step 1: Preparing the Training Corpus</h3>\n","\n","<style>\n","        .bpe-kbd-dark {\n","            background-color: #1a202c;\n","            border: 1px solid #4a5568;\n","            border-bottom: 2px solid #718096;\n","            border-radius: 4px;\n","            padding: 3px 6px;\n","            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;\n","            font-size: 0.9em;\n","            color: #cbd5e1;\n","            white-space: nowrap;\n","        }\n","    </style>\n","\n"," <h4 style=\"color: #a0aec0; margin-bottom: 5px;\">What this part does:</h4>\n","    <p style=\"color: #a0aec0;\">\n","        Before we can start merging pairs, we need a corpus of text in the right format. The BPE algorithm operates on a vocabulary built from words and their frequencies. So, our first step is to:\n","    </p>\n","    <ol style=\"color: #a0aec0; padding-left: 25px;\">\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Take a raw text corpus.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Split it into individual words and count the frequency of each unique word.</li>\n","        <li style=\"padding-left: 10px; margin-bottom: 8px;\">Represent each word as a sequence of characters, separated by spaces, and add a special end-of-word marker, like <span class=\"bpe-kbd-dark\">&lt;/w&gt;</span>.</li>\n","    </ol>\n","     <p style=\"color: #a0aec0; margin-top: 15px;\">\n","        This marker is crucial as it allows the algorithm to distinguish between a subword found inside a larger word (like <span class=\"bpe-kbd-dark\">er</span> in \"newer\") and a subword that forms the end of a word (like <span class=\"bpe-kbd-dark\">er&lt;/w&gt;</span> in \"lower\"). Our goal is to transform a block of text into a dictionary where keys are the character-split words and values are their frequencies.\n","    </p>\n","\n"," <div style=\"margin-top: 25px; background-color: #1a202c; border-left: 5px solid #805ad5; padding: 15px;\">\n","        <h4 style=\"color: #b794f4; margin: 0 0 5px 0;\">Example</h4>\n","         <p style=\"margin: 0; color: #a0aec0;\">\n","            <strong>Input Corpus:</strong> <code>\"low lower newest wider low\"</code><br>\n","            <strong>Becomes:</strong> <code>{'l o w </w>': 2, 'l o w e r </w>': 1, ...}</code>\n","        </p>\n","    </div>\n","</div>\n"]},{"cell_type":"code","execution_count":1,"id":"4f600697","metadata":{"execution":{"iopub.execute_input":"2025-10-25T10:09:27.742244Z","iopub.status.busy":"2025-10-25T10:09:27.741448Z","iopub.status.idle":"2025-10-25T10:09:27.752165Z","shell.execute_reply":"2025-10-25T10:09:27.751162Z"},"papermill":{"duration":0.015397,"end_time":"2025-10-25T10:09:27.753535","exception":false,"start_time":"2025-10-25T10:09:27.738138","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Initial Vocabulary ---\n","'l o w </w>': 3\n","'l o w e r </w>': 1\n","'n e w e s t </w>': 1\n","'w i d e r </w>': 1\n"]}],"source":["import collections\n","\n","def get_vocab(corpus: str) -> dict:\n","    \"\"\"\n","    Takes a raw text corpus and creates an initial vocabulary.\n","    \n","    The vocabulary maps each word (split into characters and with an end-of-word marker)\n","    to its frequency in the corpus.\n","\n","    Args:\n","        corpus (str): A string containing the training text.\n","\n","    Returns:\n","        dict: A dictionary where keys are space-separated characters of words\n","              (e.g., 'l o w </w>') and values are their counts.\n","    \"\"\"\n","    # Split the corpus into words and count their frequencies\n","    words = corpus.strip().split()\n","    word_counts = collections.Counter(words)\n","    \n","    # Initialize the vocabulary dictionary\n","    vocab = {}\n","    \n","    # For each word and its count, format it for BPE\n","    for word, count in word_counts.items():\n","        # Join characters with a space and add the end-of-word marker\n","        bpe_word = ' '.join(list(word)) + ' </w>'\n","        vocab[bpe_word] = count\n","        \n","    return vocab\n","\n","# --- Let's test it with our example corpus ---\n","\n","corpus_text = \"low lower newest wider low low\"\n","initial_vocab = get_vocab(corpus_text)\n","\n","print(\"--- Initial Vocabulary ---\")\n","# Use a nicer format for printing the dictionary\n","for word, count in initial_vocab.items():\n","    print(f\"'{word}': {count}\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":4.808363,"end_time":"2025-10-25T10:09:28.074045","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-25T10:09:23.265682","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}